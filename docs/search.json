[
  {
    "objectID": "posts/Alcohol/index.html#total-litres-of-pure-alcohol",
    "href": "posts/Alcohol/index.html#total-litres-of-pure-alcohol",
    "title": "Alcohol consumption",
    "section": "Total litres of pure alcohol",
    "text": "Total litres of pure alcohol\n\n\n\n\n\nSee this Figure, it shows a downtrend, and there is more than one peak in the distribution plot, which means the total litres of pure alcohol in most countries is around 1, 6.5, or 10.5.\n\n\n\n\n\n\n\n\n\nFrom this graph, we could see that the total litres of pure alcohol in Europe and Australia are almost higher than 10, especially in Belarus, the number is 14.4. By contrast, total litres of pure alcohol in Africa is almost zero. And the total litres of pure alcohol in South America and North America is approximately 8, while it in Asia is around 5."
  },
  {
    "objectID": "posts/Alcohol/index.html#global-alcohol-servings",
    "href": "posts/Alcohol/index.html#global-alcohol-servings",
    "title": "Alcohol consumption",
    "section": "Global alcohol servings",
    "text": "Global alcohol servings\n\n\n\n\n\nFrom the histogram plot of alcohol servings, there is more number of countries that barely do not consume wine, the number of countries that do not serve spirit is second, and the number of countries that do not consume beer is least. And from the density plot of alcohol servings, we could see beer has more consumption compared with wine and spirit when consumption is bigger than 200. And wine has more small servings.\n\n\n\n\nMean and median of alcohol servings\n \n  \n    alcohol servings \n    mean \n    median \n  \n \n\n  \n    beer servings \n    106.16 \n    76 \n  \n  \n    spirit servings \n    80.99 \n    56 \n  \n  \n    wine servings \n    49.45 \n    8 \n  \n\n\n\n\n\nFrom the mean and median of alcohol servings table, we could easily know that beer has the highest servings, while the spirit is the second one and wine is the least one."
  },
  {
    "objectID": "posts/Alcohol/index.html#relationship-between-total-litres-of-pure-alcohol-and-alcohol-types",
    "href": "posts/Alcohol/index.html#relationship-between-total-litres-of-pure-alcohol-and-alcohol-types",
    "title": "Alcohol consumption",
    "section": "Relationship between total litres of pure alcohol and alcohol types",
    "text": "Relationship between total litres of pure alcohol and alcohol types\n\n\n\n\n\n\n\n\n\nFrom this figure, we can see that there is a barrier in this plot, upper than the red line, and the two blue lines indicate that servings which the total litres of pure alcohol less than 5 are under 200, which means it is difficult to have high servings with low total litres of pure alcohol.\n\n\n\n\n\n\n\n\n\nSee this figure, it is obvious that there is a positive model in beer servings, which means total litres of pure alcohol are higher, and the beer servings are higher. For spirit servings and wine servings, most of the servings are under 200 and 100 respectively."
  },
  {
    "objectID": "posts/Alcohol/index.html#dataset",
    "href": "posts/Alcohol/index.html#dataset",
    "title": "Alcohol Consumption",
    "section": "Dataset",
    "text": "Dataset\nMore data can be found here.\n\n\nCode\nalcohol <- alcohol %>%\n  rename('beer servings' = beer_servings,\n         'spirit servings' = spirit_servings,\n         'wine servings' = wine_servings,\n         'total litres of pure alcohol' = total_litres_of_pure_alcohol)\n\nDT::datatable(alcohol, \n              options = list(pageLength = 5), \n              caption = \"Alcohol consumption by country\",\n              text_spec(alcohol, font_size = \"small\") \n             \n             )"
  },
  {
    "objectID": "posts/Alcohol/index.html#reference",
    "href": "posts/Alcohol/index.html#reference",
    "title": "Alcohol Consumption",
    "section": "Reference",
    "text": "Reference\nThomas Mock (2022). Tidy Tuesday: A weekly data project aimed at the R ecosystem https://github.com/rfordatascience/tidytuesday.\nAuguie, B (2017). gridExtra: Miscellaneous Functions for “Grid” Graphics. R package version 2.3. https://CRAN.R-project.org/package=gridExtra.\nPedersen, TL (2020). patchwork: The Composer of Plots. R package version 1.1.1. https://CRAN.R-project.org/package=patchwork.\nR Core Team (2022). R: A Language and Environment for Statistical Computing. R Foundation for Statistical Computing. Vienna, Austria. https://www.R-project.org/.\nSievert, C (2020). Interactive Web-Based Data Visualization with R, plotly, and shiny. Chapman and Hall/CRC. https://plotly-r.com.\nWickham, H, M Averick, J Bryan, W Chang, LD McGowan, R François, G Grolemund, A Hayes, L Henry, J Hester, M Kuhn, TL Pedersen, E Miller, SM Bache, K Müller, J Ooms, D Robinson, DP Seidel, V Spinu, K Takahashi, D Vaughan, C Wilke, K Woo, and H Yutani (2019). Welcome to the tidyverse. Journal of Open Source Software 4(43), 1686.\nXie, Y (2022). knitr: A General-Purpose Package for Dynamic Report Generation in R. R package version1.38. https://yihui.org/knitr/.\nXie, Y, J Cheng, and X Tan (2022). DT: A Wrapper of the JavaScript Library ‘DataTables’. R package version 0.24. https://CRAN.R-project.org/package=DT.\nZhu, H (2021). kableExtra: Construct Complex Table with ‘kable’ and Pipe Syntax. R package version 1.3.4. https://CRAN.R-project.org/package=kableExtra.\nWikipedia.(2022).Alcoholic drink. https://en.wikipedia.org/wiki/Alcoholic_drink.\nTHE WORLD BANK.(2022).Total alcohol consumption per capita (liters of pure alcohol, projected estimates, 15+ years of age). https://data.worldbank.org/indicator/SH.ALC.PCAP.LI"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Di Cui",
    "section": "",
    "text": "This blog shows Di Cui’s learning results."
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!"
  },
  {
    "objectID": "posts/Alcohol/index.html#introduction",
    "href": "posts/Alcohol/index.html#introduction",
    "title": "Alcohol Consumption",
    "section": "Introduction",
    "text": "Introduction\nAlcohol’s role in our lives extends beyond a mere drink. It mirrors alcoholism, cultural facets, and economic dynamics. This blog delves into global alcohol consumption, spotlighting beer, spirits, and wine. These choices echo traditions, camaraderie, and refinement. Beer unites at gatherings, spirits offer mystique, and wine embodies elegance. From local pubs to international soirées, alcohol reflects diverse human experiences. This exploration unravels trends and stories, showcasing the interplay of culture and economics in our relationship with alcohol.\n\n\n image source: Wikipedia"
  },
  {
    "objectID": "posts/Alcohol/index.html#data-description",
    "href": "posts/Alcohol/index.html#data-description",
    "title": "Alcohol Consumption",
    "section": "Data description",
    "text": "Data description\nThis data shows the beer, spirit and wine consumption and total litres of pure alcohol by country. The data source is from Thomas Mock (2022). Tidy Tuesday: A weekly data project aimed at the R ecosystem https://github.com/rfordatascience/tidytuesday.\nVariables:\n\ncountry : global country name.\nbeer_servings : consumption of beer.\nspirit_servings : consumption of spirit.\nwine-servings : consumption of wine.\ntotal_litres_of_pure_alcohol : the total (sum of recorded and unrecorded alcohol) amount of alcohol consumed per person (15 years of age or older) over a calendar year, in litres of pure alcohol, adjusted for tourist consumption. (The World Bank Group,2022)\n\nClick here to see or download the data."
  },
  {
    "objectID": "posts/Alcohol/index.html#relationship-between-total-litres-of-pure-alcohol-and-alcohol-servings",
    "href": "posts/Alcohol/index.html#relationship-between-total-litres-of-pure-alcohol-and-alcohol-servings",
    "title": "Alcohol consumption",
    "section": "Relationship between total litres of pure alcohol and alcohol servings",
    "text": "Relationship between total litres of pure alcohol and alcohol servings\n\n\n\n\n\nFrom this figure, we can see that there is a barrier above the red line in this plot, and the two blue lines show that servings which the total litres of pure alcohol less than 5 are under 200, which indicates that it is difficult to have high servings for low total litres of pure alcohol.\n\n\n\n\n\n\n\n\n\nSee this figure, it is obvious that there is a positive relationship in beer servings plot, which means total litres of pure alcohol are higher, and the beer servings are higher. For spirit servings and wine servings, most of the servings are under 200 and 100 respectively."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Di Cui’s blog",
    "section": "",
    "text": "The Gini index across US states analysis\n\n\n\n\n\n\n\nHigh Dimensional Data Analysis\n\n\n\n\n\n\n\n\n\n\n\nSep 1, 2023\n\n\nDi Cui\n\n\n\n\n\n\n  \n\n\n\n\nHandwriting Analysis\n\n\n\n\n\n\n\nMachine Learning\n\n\n\n\n\n\n\n\n\n\n\nMay 15, 2023\n\n\nDi Cui\n\n\n\n\n\n\n  \n\n\n\n\nAlternative Fuel Stations\n\n\n\n\n\n\n\nExploration Data Analysis\n\n\n\n\n\n\n\n\n\n\n\nNov 25, 2022\n\n\nDi Cui\n\n\n\n\n\n\n  \n\n\n\n\nAlternative Fuel Stations\n\n\n\n\n\n\n\nExploration Data Analysis\n\n\n\n\n\n\n\n\n\n\n\nNov 25, 2022\n\n\nDi Cui\n\n\n\n\n\n\n  \n\n\n\n\nNobel Prize laureates\n\n\n\n\n\n\n\nCommunicating With Data\n\n\n\n\n\n\n\n\n\n\n\nOct 16, 2022\n\n\nDi Cui\n\n\n\n\n\n\n  \n\n\n\n\nAlcohol Consumption\n\n\n\n\n\n\n\nCommunicating With Data\n\n\n\n\n\n\n\n\n\n\n\nAug 29, 2022\n\n\nDi Cui\n\n\n\n\n\n\n  \n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\n\nWelcome\n\n\n\n\n\n\n\n\n\n\n\nAug 28, 2022\n\n\nDi Cui\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/Alcohol/index.html#global-alcohol-consumption",
    "href": "posts/Alcohol/index.html#global-alcohol-consumption",
    "title": "Alcohol Consumption",
    "section": "Global alcohol consumption",
    "text": "Global alcohol consumption\n\n\nCode\nmap <- map_data(\"world\") \n\nmap$region <- gsub(\"Russia\", \"Russian Federation\", map$region)\n\nmap_data <- map %>%\n  left_join(alcohol, by = c(\"region\"= \"country\")) \n\n p1<-ggplot(map_data, aes(x = long, y = lat, group = group, \n                     text = sprintf(\"region: %s<br>total litres of pure alcohol: %s\",\n                                    region, total_litres_of_pure_alcohol)))+\n geom_polygon( aes(fill = total_litres_of_pure_alcohol), color = \"white\")+\n  scale_fill_gradient(low = \"#f7fcfd\", \n                        high = \"#4d004b\",\n                      na.value = \"grey\",\n                      breaks = c(0,2,4,6,8,10,12,14),\n                      labels = c(0,2,4,6,8,10,12,14))+\n  theme_void()+\n  theme(panel.grid = element_blank(),\n        axis.line.x = element_blank(),\n        axis.line.y = element_blank())+\n  labs(fill = \"Litres\",\n       title = \"Global alcohol consumption\")\n\n\n\n\n\n\nCode\nggplotly(p1,tooltip = \"text\")\n\n\n\n\n\n\n\nFigure1: Global alcohol consumption (total litres of pure alcohol). Note: NA value: grey. Litres: total litres of pure alcohol\n\n\nFrom Figure1, we could see that alcohol consumption in Europe and Australia is almost higher than 10. By contrast, alcohol consumption in Africa is close to zero. And the consumption in South America and North America is approximately 8, while it in Asia is around 5."
  },
  {
    "objectID": "posts/Alcohol/index.html#global-alcohol-consumption-of-beer-spirit-and-wine",
    "href": "posts/Alcohol/index.html#global-alcohol-consumption-of-beer-spirit-and-wine",
    "title": "Alcohol Consumption",
    "section": "Global alcohol consumption of beer, spirit and wine",
    "text": "Global alcohol consumption of beer, spirit and wine\n\n\n\n\nCode\nalcohol_table <- tibble('alcohol servings' = c(\"beer servings\",\n                                               \"spirit servings\",\n                                               \"wine servings\"),\n                        mean = c(round(mean(alcohol$beer_servings),2),\n                                 round(mean(alcohol$spirit_servings),2),\n                                 round(mean(alcohol$wine_servings),2)\n                                ),\n                        median = c(median(alcohol$beer_servings),\n                                   median(alcohol$spirit_servings),\n                                   median(alcohol$wine_servings)),\n                        sum = c(sum(alcohol$beer_servings),\n                                   sum(alcohol$spirit_servings),\n                                   sum(alcohol$wine_servings))\n                        )\n  \n alcohol_table %>%\n   knitr::kable(caption = \"Table1: Mean, median and sum of beer servings, spirit servings and wine servings\") %>%\n   kable_styling()\n\n\n\n\nTable1: Mean, median and sum of beer servings, spirit servings and wine servings\n \n  \n    alcohol servings \n    mean \n    median \n    sum \n  \n \n\n  \n    beer servings \n    106.16 \n    76 \n    20489 \n  \n  \n    spirit servings \n    80.99 \n    56 \n    15632 \n  \n  \n    wine servings \n    49.45 \n    8 \n    9544 \n  \n\n\n\n\n\n\nFrom Table1, one thing we need to note that is we cannot compare the consumption by alcohol servings without a unit. We could easily know that beer consumption in a country is normally around 106 and the total beer consumption is around 20 thousand, and the average spirit consumption is around 80 and the total spirit consumption is around 15 thousand, and the average wine consumption is approximately 50 and the total wine consumption is around 9 thousand.\n\nGlobal beer consumption\n\n\nCode\np2<-ggplot(map_data, aes(x = long, y = lat, group = group, \n                     text = sprintf(\"region: %s<br>beer servings: %s\",\n                                    region, beer_servings)))+\n  geom_polygon( aes(fill = beer_servings), color = \"white\")+\n  scale_fill_gradient(low = \"#fff7f3\", \n                        high = \"#49006a\",\n                      na.value = \"grey\",\n                      breaks = c(0,50,100,150,200,250,300,350,400),\n                      labels = c(0,50,100,150,200,250,300,350,400))+\n  theme_void()+\n  theme(panel.grid = element_blank(),\n        axis.line.x = element_blank(),\n        axis.line.y = element_blank())+\n  labs(fill = \"Servings\",\n       title = \"Beer global alcohol consumption\")\n\n\n\n\n\n\nCode\nggplotly(p2, tooltip = \"text\")\n\n\n\n\n\n\n\nFigure2: Global beer consumption. Note: NA value: grey. Servings: beer servings\n\n\nSee Figure2, beer consumption in northern Africa is almost under 50, and the consumption in southwestern Africa is almost high, with most countries over 200, and even near 350. Beer servings in Europe, North America, South America and Australia are almost higher than 200, especially consumption in Europe, some countries exceed 300, while servings in Asia are all approximately under 100.\n\n\nGlobal spirit consumption\n\n\nCode\np3<- ggplot(map_data, aes(x = long, y = lat, group = group, \n                     text = sprintf(\"region: %s<br>spirit servings: %s\",\n                                    region, spirit_servings)))+\n  geom_polygon( aes(fill = spirit_servings), color = \"white\")+\n  scale_fill_gradient(low = \"#fff7f3\", \n                        high = \"#49006a\",\n                      na.value = \"grey\" ,\n                      breaks = c(0,50,100,150,200,250,300,350,400),\n                      labels = c(0,50,100,150,200,250,300,350,400))+\n  theme_void()+\n  theme(panel.grid = element_blank(),\n        axis.line.x = element_blank(),\n        axis.line.y = element_blank())+\n  labs(fill = \"Servings\",\n       title = \"Spirit global alcohol consumption\",\n       caption = \"NA value: grey;\n       Servings: spirit servings\")\n\n\n\n\n\n\nCode\nggplotly(p3, tooltip = \"text\")\n\n\n\n\n\n\n\nFigure3: Global spirit consumption. Note: NA value: grey. Servings: spirit servings\n\n\nIn Figure3, the consumption of spirit is highest in Eastern Europe, for example, Russian Federation and Belarus consume more than 300. And the consumption in Asia is around 200, in Africa, it is almost close to zero.\n\n\nGlobal wine consumption\n\n\nCode\np4 <- ggplot(map_data, aes(x = long, y = lat, group = group, \n                     text = sprintf(\"region: %s<br>wine servings: %s\",\n                                    region, wine_servings)))+\n geom_polygon( aes(fill = wine_servings), color = \"white\")+\n  scale_fill_gradient(low = \"#fff7f3\", \n                        high = \"#49006a\",\n                      na.value = \"grey\",\n                      breaks = c(0,50,100,150,200,250,300,350,400),\n                      labels = c(0,50,100,150,200,250,300,350,400))+\n  theme_void()+\n  theme(panel.grid = element_blank(),\n        axis.line.x = element_blank(),\n        axis.line.y = element_blank())+\n  labs(fill = \"Servings\",\n       title = \"Wine global alcohol consumption\",\n       caption = \"NA value: grey;\n       Servings: wine servings\")\n\n\n\n\n\n\nCode\nggplotly(p4, tooltip = \"text\")\n\n\n\n\n\n\n\nFigure4: Global wine consumption. Note: NA value: grey. Servings: wine servings\n\n\nFrom Figure4, it is easy to see that global wine consumption is not high, except in southern South America, Australia and western Europe, including Argentina, France and Australia, where the consumption is greater than 200."
  },
  {
    "objectID": "posts/Alcohol/index.html#global-beer-consumption",
    "href": "posts/Alcohol/index.html#global-beer-consumption",
    "title": "Alcohol consumption",
    "section": "Global beer consumption",
    "text": "Global beer consumption\n\n\n\n\n\n\n\n\n\nNA value: grey; Servings: beer servings\nSee Figure3, beer consumption in Northern Africa is almost under 50, and the consumption in Southwestern Africa is almost high, with most countries over 200, and even near 350. Beer servings in Europe, North America, South America and Australia are almost higher than 200, especially consumption in Europe, some countries exceed 300, while servings in Asia are all approximately under 100."
  },
  {
    "objectID": "posts/Alcohol/index.html#global-spirit-consumption",
    "href": "posts/Alcohol/index.html#global-spirit-consumption",
    "title": "Alcohol consumption",
    "section": "Global spirit consumption",
    "text": "Global spirit consumption\n\n\n\n\n\n\n\n\n\nNA value: grey; Servings: spirit servings\nIn Figure4, the consumption of spirit is highest in Eastern Europe, for example, Russian Federation and Belarus consume more than 300. And the consumption in Asia is around 200, in Africa, it is almost close to zero."
  },
  {
    "objectID": "posts/Alcohol/index.html#global-wine-consumption",
    "href": "posts/Alcohol/index.html#global-wine-consumption",
    "title": "Alcohol consumption",
    "section": "Global wine consumption",
    "text": "Global wine consumption\n\n\n\n\n\n\n\n\n\nNA value: grey; Servings: wine servings\nFrom Figure5, it is easily to see that global wine consumption is not high, except in southern South America, Australia and western Europe, including Argentina, France and Australia, where the consumption is greater than 200."
  },
  {
    "objectID": "posts/Nobel/index.html#introduction",
    "href": "posts/Nobel/index.html#introduction",
    "title": "Nobel Prize laureates",
    "section": "Introduction",
    "text": "Introduction\nThe recent announcement of the Nobel Prize winners, which runs from October 3, has attracted a lot of attention. Nobel Prize is a set of annual international awards bestowed in several categories by Swedish and Norwegian institutions in recognition of academic, cultural, or scientific advances. Nobel Prizes are awarded in the fields of Physics, Chemistry, Physiology or Medicine, Literature, and Peace. This blog gives an overview of Nobel winners and Nobel laureate publications from 1901 to 2016.\n\n\n Image source: The Nobel Prize medal. Photo: Alexander Mahmoud 2018"
  },
  {
    "objectID": "posts/Nobel/index.html#data-description",
    "href": "posts/Nobel/index.html#data-description",
    "title": "Nobel Prize laureates",
    "section": "Data description",
    "text": "Data description\nThere are two datasets used in the blog, one covers all of the Nobel prize winners records from 1901 to 2016 (i.e. name, birth date, prize year, category, gender and organization name), and the other one covers all the publication records for almost all Nobel laureates in physics, chemistry, and medicine from 1901 to 2016 (i.e. title, publication year and category).\nThe data source is from Thomas Mock (2022). Tidy Tuesday: A weekly data project aimed at the R ecosystem https://github.com/rfordatascience/tidytuesday.\nClick here to see or download the data."
  },
  {
    "objectID": "posts/Nobel/index.html#nobel-winners",
    "href": "posts/Nobel/index.html#nobel-winners",
    "title": "Nobel Prize laureates",
    "section": "Nobel winners",
    "text": "Nobel winners\n\nCount of Nober winners\nBetween 1901 and 2016, the Nobel Prizes were awarded to 903 people and organisations: 877 individuals and 26 organisations. Below, you can view the count of Nobel Prizes winners over year.\n\n\n\n\n\n\n\n\n\n\n\n\nFigure1: Count of Nobel winners over year from 1901 to 2016.\n\n\nSee Figure1. Since 1901, there have been Nobel Prize winners almost every year, and the number of winners has been rising, although the rise has been accompanied by ups and downs. It is worth noting that the number of laureates declined during World War I (1914-1918) and World War II (1939-1945), especially the precipitous decline during World War I and the absence of laureates between 1939 and 1943. It may be because of the economic downturn caused by the war and the deficit of the Nobel Prize.\n\n\nGender of Nobel winners\nBetween 1901 and 2016, the Nobel Prizes were awarded to 48 females and 833 males. Below, you can view the count of Nobel Prizes winners over year by gender.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure2: Count of Nobel winners over year by gender from 1901 to 2016.\n\n\nSee Figure2. The number of female Nobel Prize winners is far less than that of male, it was 0 or 1 until 1976, when the number of female Nobel Prize winners exceeded 1, and then the number of female winners has shown rapid growth, especially in 2009, when 6 were achieved. The status of women in the Nobel Prize world is changing. As education becomes more widespread，it is expected that as women around the world become more educated, they will achieve more in the Nobel Prize.\n\n\nNobel winners’ prize age\n\n\n\nBetween 1901 and 2016, the youngest laureate is 17, and the oldest laureate is 90. Below, you can view the age distribution of Nobel laureates at the time of the award.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure3: Age distribution of Nobel laureates at the time of the award.\n\n\nSee Figure3. Awards at the age of 17 and 90 are rare, the majority of Nobel laureates were between 50 and 70 years old at the time of their award.\nProbably because of the war, during World War I (1914-1918) and World War II (1939-1945), the average age at the time of the award was lower than in other years. The red line represents the age of 60. Before 1966, the average age of most of the annual winners was under 60 at the time of the award, and after 1966, most of the average age were over 60 years old when they won the prize, which may be the reason for the increase in the number of winners and their longevity."
  },
  {
    "objectID": "posts/Nobel/index.html#nobel-laureate-publications",
    "href": "posts/Nobel/index.html#nobel-laureate-publications",
    "title": "Nobel Prize laureates",
    "section": "Nobel laureate publications",
    "text": "Nobel laureate publications\nA person or organisation awarded the Nobel Prize is called Nobel Prize laureate.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure4: Count of Nobel Laureate Publications over year by category from 1901 to 2016.\n\n\nSee Figure4. The overall trend in the development of the three categories of publications was similar, declining during World War II (1939-1945) and then slowly pulling away from each other, with chemistry taking first place, medicine second, and physics third. Around 1996, the number of publications in these three categories declined, but surprisingly, the number of publications in chemistry reached about 1500 in 2010, almost three times as many as in medicine and five times as many as in physics."
  },
  {
    "objectID": "posts/Nobel/index.html#conclusion",
    "href": "posts/Nobel/index.html#conclusion",
    "title": "Nobel Prize laureates",
    "section": "Conclusion",
    "text": "Conclusion\nSince 1901, the Nobel Prize has become increasingly important in the world and has become a highly influential academic prize in the world.\nWith the rapid development of science and technology as well as the popularity of education, more and more people are winning the Nobel Prize, as well as the winners also publish more and more publications, the results of the Nobel Prize and the publications of the winners help the world to progress. We expect more and more people to go further and further on the path of science, technology and literature in the future."
  },
  {
    "objectID": "posts/Nobel/index.html#reference",
    "href": "posts/Nobel/index.html#reference",
    "title": "Nobel Prize laureates",
    "section": "Reference",
    "text": "Reference\nThomas Mock (2022). Tidy Tuesday: A weekly data project aimed at the R ecosystem https://github.com/rfordatascience/tidytuesday.\nPedersen, TL (2020). patchwork: The Composer of Plots. R package version 1.1.1. https://CRAN.R-project.org/package=patchwork.\nSievert, C (2020). Interactive Web-Based Data Visualization with R, plotly, and shiny. Chapman and Hall/CRC. https://plotly-r.com.\nGrolemund, G and H Wickham (2011). Dates and Times Made Easy with lubridate. Journal of Statistical Software 40(3), 1–25.\nWickham, H, M Averick, J Bryan, W Chang, LD McGowan, R François, G Grolemund, A Hayes, L Henry, J Hester, M Kuhn, TL Pedersen, E Miller, SM Bache, K Müller, J Ooms, D Robinson, DP Seidel, V Spinu, K Takahashi, D Vaughan, C Wilke, K Woo, and H Yutani (2019). Welcome to the tidyverse. Journal of Open Source Software 4(43), 1686.\nTHE NOBEL PRIZE.(2022).NOBEL PRIZES AND LAUREATES https://www.nobelprize.org/."
  },
  {
    "objectID": "posts/Alcohol Shiny App/index.html",
    "href": "posts/Alcohol Shiny App/index.html",
    "title": "Alcohol Shiny App",
    "section": "",
    "text": "Shiny applications not supported in static R Markdown documents"
  },
  {
    "objectID": "posts/Elec Fuel/index.html#data-description",
    "href": "posts/Elec Fuel/index.html#data-description",
    "title": "Alternative Fuel Stations",
    "section": "Data description",
    "text": "Data description\nThe U.S. Department of Energy collects this data in partnership with Clean Cities coalitions and their stakeholders to help fleets and consumers find alternative fueling stations.[1] This data is available at https://data-usdot.opendata.arcgis.com/datasets/usdot::alternative-fueling-stations/about. The collection of this data is like a census, which is collected over time.\nThe dataset has 70 variables and 59,927 observations, containing character, numeric, and date type variable, which could lead us to find station’s location, station’s status, station’s access, and alternative fuel infrastructure and more information in stations in the dataset.\nThe type of alternative fuel the station provides, includes Biodiesel (B20 and above), Compressed Natural Gas (CNG), Electric, Ethanol (E85), Hydrogen, Liquefied Natural Gas (LNG), Propane (LPG).\nImportant variables:\n\n\n\n\n\n\n\n \n  \n    Variables (Fields) \n    Type \n    Description \n  \n \n\n  \n    state \n    string \n    The two character code for the U.S. state or Canadian province/territory of the station's location. \n  \n  \n    ev_level1_evse_num \n    integer \n    For electric stations, the number of Level 1 EVSE ports. \n  \n  \n    ev_level2_evse_num \n    integer \n    For electric stations, the number of Level 2 EVSE ports. \n  \n  \n    field_posts \n    array \n    An array of strings containing the identifier(s) for the post(s) provided by the EV network, if available. \n  \n  \n    objectid \n    integer \n    A unique identifier for this specific station. \n  \n  \n    fuel_type_code \n    string \n    The type of alternative fuel the station provides. \n  \n  \n    station_name \n    string \n    The name of the station."
  },
  {
    "objectID": "posts/Elec Fuel/index.html#how-does-the-number-of-electric-stations-change-by-state-over-year",
    "href": "posts/Elec Fuel/index.html#how-does-the-number-of-electric-stations-change-by-state-over-year",
    "title": "Alternative Fuel Stations",
    "section": "How does the number of electric stations change by state over year?",
    "text": "How does the number of electric stations change by state over year?\n\n\n\n\n\nFigure2: The trend of electric stations over year by state. Note: The y-axis is log transformed.\n\n\n\n\n\n\n\n\n\nFigure3: The trend of electric stations over year in the four specimen states. Note: The y-axis is log transformed.\n\n\n\n\nTo analyze this question, we calculate the sum of electric stations which have opened before 2022 by state over year. See Figure2, only California (CA) opened electric stations before 2007, and while most states started to open electric stations after 2008. Furthermore, they are also leading state to install alternative fuel stations, potentially was Methanol fuel ports as we have discussed in the introduction part. Interestingly, during that period of 1990s, not a single state have these Methanol fuel stations installed at all.\nFrom this point of view we can imply that California is highly innovative and experimental, especially it can be further perceived that California state’s citizen are extremely environmentalist and Eco-friendly mindset.\nFurthermore, for more meaningful analysis, states of California (CA), New York (NY), Alaska (AK), North Dakota (ND), Ontario (ON), and Puerto Rico (PR) are selected as specimens to analyse in a comparative and contrasting methodology. See Figure3, CA and NY started to open electric stations in 1997 and in 2008 respectively, and the number of electric stations in CA and NY keeps growing and surpasses 100 stations rapidly and sustain-ably. These two states are particularly known to be a “big” state with major capitals, hence in some ways it is expected to see CA and NY to be big early mover for innovations for Eco-friendly and sustainability technology.\nAccording to CBS, New York has been pushing for electric stations and charging ports heavily recently. As many people are starting to purchase more of electric car due to the fact that it made them feel good about the reduction of emissions which is significantly lower than relying on gasoline and internal combustion engine. However, the reason that California has it leads to large margin in terms of electric stations installed is because of rules and regulations of mandating the ownership of electric car, along with multiple incentive, namely price discount and tax refund. Hence these reasons are truly justified why these two states are having a great extensive number of electric stations, and both California and New York states should be ideal examples for other states to follow.\nOn the other hand, AK and ND, both of which are supposedly smaller states in the USA, are undoubtedly having less number of electric stations and open most recently as 2015 and 2011 respectively. It is understandable that perhaps electric vehicles’ accessibility for people in these two states could be much lower. Nevertheless, both AK and ND should be admirable for having stable number of electric stations open in the last 4 years.\nOn the other extreme ends, ON and PR have just started to open electric station in 2021 with very few number of stations as well. Even though they are territories in Canada and presumably smaller states in comparison to other states, they still have to follow other states and lift increasing in awareness and promotion of electric vehicles.\nOverall, the United States and Canadian territories are in a healthy trend of opening new alternative fuel stations with more and more encouragement and awareness toward larger group of people."
  },
  {
    "objectID": "posts/Elec Fuel/index.html#does-the-number-of-electric-stations-have-a-effect-on-the-number-of-ports",
    "href": "posts/Elec Fuel/index.html#does-the-number-of-electric-stations-have-a-effect-on-the-number-of-ports",
    "title": "Alternative Fuel Stations",
    "section": "Does the number of electric stations have a effect on the number of ports ?",
    "text": "Does the number of electric stations have a effect on the number of ports ?\n\n\n\n\n\nFigure4: The relationship between ports and ELEC stations over year. Note: The y-axis is log transformed.\n\n\n\n\n\n\n\n\n\nFigure5: The trend of proporation(ports/stations) over year by state. Note: The y-axis is log transformed.\n\n\n\n\n\n\n\n\n\nFigure6: The trend of proporation(ports/stations) over year in the four states. Note: The y-axis is log transformed.\n\n\n\n\nTo see the relationship between the number of electric ports and the number of electric stations, we calculated the ports number per station provides over year as Figure4, in which it shows that proportion of ports number and stations number shows a downtrend over year overall, then the number is stabilized after 2012 in a slightly downward trend within the range of mere 1 station.\nHowever, the key takeaway from this plot is trend of number of ports which is in a decreasing pattern instead of increasing, with the growth of number of electric stations. We do found this very counter-intuitive, surprising yet interesting.\nHence the reason why we want to further focus on the proportion in each state. See Figure5, the proportion in most of states is in the range of 2 and 3 with a more stabilized trend to slightly downward trend.\nReferring to Figure6, we have chosen interesting states with extremely different value for the sake of clear comparison, namely California (CA), Ontario (ON), Utah (UT), Wyoming (WY). The proportion of CA reached the peak which is around 30 during 1999, and the proportion of ON, UT, WY states has value under 1, which means there are some electric stations cannot even provide one port and even uninstall in during the particular year.\nOur preliminary assumption is that electric charging ports are becoming more privatized and stray away from traditional fuel stations. For example, in the case of Tesla motor which encourage to cut the middle man both vehicle sales and charging ports station - both of which are funded by the company. All of this is to make electric vehicles more accessible and charging ports more convenient and accessible for majority of their customer and fellow electric car user alike.\nAccording to Hawkins, these chargers are becoming more and more accessible and thoroughly installed with more option of charger like “Supercharger” and compatibility beyond Tesla vehicle only. Hence, these non-fuel-station-tied charger proved to be more versatile and flexible in comparison to traditional ports available at fuel station. By seeing this trend, it proved to be sensible and understandable that this could be potential new trends for the near future."
  },
  {
    "objectID": "posts/Elec Fuel/index.html#electric-stations-and-port-number-during-1995-2021",
    "href": "posts/Elec Fuel/index.html#electric-stations-and-port-number-during-1995-2021",
    "title": "Alternative Fuel Stations",
    "section": "Electric stations and port number during 1995-2021",
    "text": "Electric stations and port number during 1995-2021"
  },
  {
    "objectID": "posts/Machine Learning/index.html",
    "href": "posts/Machine Learning/index.html",
    "title": "Handwriting Analysis",
    "section": "",
    "text": "The analysis of handwriting has always held a captivating allure, offering insights into the distinctive subtleties of human expression. Even in today’s digital age, the study of handwriting remains relevant, finding applications in fields like forensic document examination, linguistics, and personalized marketing. This report delves into the realm of handwritten letter classification by employing a combination of Principal Component Analysis (PCA) and machine learning methodologies. By harnessing these techniques, our aim is to unravel the unique variations in individuals’ letter-writing styles, ultimately contributing to a deeper comprehension of the intricate art of handwriting."
  },
  {
    "objectID": "posts/Machine Learning/index.html#data-description",
    "href": "posts/Machine Learning/index.html#data-description",
    "title": "Handwriting Analysis",
    "section": "Data Description",
    "text": "Data Description\nThe data is based on the EMNIST dataset that contains a 28x28 pixel image of a letter from the 26-letter Roman alphabet. You can find details about this dataset in this article.\nMy dataset is assigned a single random letter and a random subset from this dataset. Separate to this main data, the dataset will also have a new records dataset that contains five observations.\nEach row of the data is an image. Each column corresponds to one pixel value. There are 28 x 28 = 784 columns in total. The image is a single letter with a mix of upper case and lower case."
  },
  {
    "objectID": "posts/Machine Learning/index.html#preliminary-analysis",
    "href": "posts/Machine Learning/index.html#preliminary-analysis",
    "title": "Handwriting Analysis",
    "section": "Preliminary analysis",
    "text": "Preliminary analysis\n\n\n\n\nQ1 What is the letter in your data?\nThe letter in my data is ‘y’.\n\n\nQ2 Plot a random sample of 12 images of your data with the correct orientation.\n\n\n\n\n\n\n\nQ3 Perform a principal component analysis (PCA) on your data. How much variation does the first 5 principal components explain in the data?\n\n\nImportance of first k=5 (out of 784) components:\n                            PC1      PC2       PC3       PC4       PC5\nStandard deviation     667.3433 558.4448 520.02544 418.33520 408.85848\nProportion of Variance   0.1435   0.1005   0.08712   0.05638   0.05385\nCumulative Proportion    0.1435   0.2439   0.33105   0.38742   0.44128\n\n\nWe can see that the first five principal components explain 44.128% of the variation in the data, and PC1, PC2, PC3, PC4 and PC5 explain 14.35%, 10.05%, 8.712%, 5.638% and 5.385% of the variation in the data respectively.\n\n\nQ4 Show what aspect of the data the first and second principal component loadings capture.\n\n\n\n\n\n\n\n\n\n\nQ5 Using the rotated data from the PCA, perform an agglomerative hierarchical clustering with average linkage.\n\n\n\nCall:\nhclust(d = dist(mydata_pca$x), method = \"average\")\n\nCluster method   : average \nDistance         : euclidean \nNumber of objects: 3448 \n\n\n\n\nQ6 Cut the tree from question 5 to 4 clusters. Show how many observations you have per cluster.\n\n\n\n\nHow many observations in the 4 clusters.\n \n  \n    cave \n    Freq \n  \n \n\n  \n    1 \n    3441 \n  \n  \n    2 \n    4 \n  \n  \n    3 \n    2 \n  \n  \n    4 \n    1 \n  \n\n\n\n\n\n\n\nQ7 Show a sample of 10 (or the total number of images in a cluster if less than 10 observations in a cluster) images from each cluster like the plot below. What do you notice about the cluster groups?\n\n\n\n\n\nWe could see that the tail of the letter ‘y’ seems to be straight and the upper part more like ‘v’ in cluster 1, while the tail of the letter ‘y’ is more curved and the upper part of the letter ‘y’ is more like ‘u’ in cluster 2. Then in cluster 3, the upper part of the letter ‘y’ is narrower in width than the lower part, and finally, in cluster 4 the upper part of the letter ‘y’ is like ‘u’ and the lower part of the letter ‘y’ is like ‘v’. And the imbalance of clusters, majority of images are in one cluster, the other 3 clusters are smaller."
  },
  {
    "objectID": "posts/Machine Learning/index.html#report",
    "href": "posts/Machine Learning/index.html#report",
    "title": "Handwriting Analysis",
    "section": "Report",
    "text": "Report\n\nAbstract\nHandwriting analysis has consistently captivated researchers, offering a window into the intricate details of human expression. Despite the prevalence of digital communication, handwriting remains pertinent, finding utility in diverse domains such as forensic document analysis, linguistics, and personalized marketing. This report explores the landscape of handwritten letter classification through the integration of Principal Component Analysis (PCA) and advanced machine learning techniques. By harnessing these methodologies, our objective is to decode the distinct variations present in how individuals write letters, thereby enhancing our understanding of the nuanced world of handwriting.\n\n\nChoose pricipal components\nTo explore the way each person writes letters and to categorize the way they write them. I do a dimension reduction to the pixel data of a letter. And then, I produce a scree plot, and the elbow appears around the fourth to seventh PC (Principal component), therefore I choose 5 PCs (Principal components) to be used to analyze, and the 5 PCs (Principal components) explain 44.12% of the variation in the data.\n\n\n\n\n\nImportance of first k=5 (out of 784) components:\n                            PC1      PC2       PC3       PC4       PC5\nStandard deviation     667.3433 558.4448 520.02544 418.33520 408.85848\nProportion of Variance   0.1435   0.1005   0.08712   0.05638   0.05385\nCumulative Proportion    0.1435   0.2439   0.33105   0.38742   0.44128\n\n\n\n\nClassify data\nI choose the k-means method to divide the different ways that a person can write a particular letter into 3 clusters. This plot shows that the letter ‘y’ is more centered in cluster 1, and the letter ‘y’ is italic, the letter tends to lean to the right in cluster 2, and the letter ‘y’ is fatter and the upper part of the letter ‘y’ resembles a ‘u’ in cluster 3.\n\n\n\n\n\n\n\n\n\n\nClassify new records\nFor classifying a set of new record of 5 observations, I use 3 supervised learning methods, and I set principal components as predictors.\n\n\n\nI create three models by using the kNN method, the Support vector classifier method with Polynomial kernel and the Random forest method respectively to classify the new 5 observations.\n\nkNN method\n\n\n\n\n\n\n\nSupport vector classifier method with Polynomial kernel\n\n\n\n\n\n\n\nRandom forest method\n\n\n\n\n\n\nWe can see that the classified results following the three models are the same and sensible, we could easily see that these are different 3 groups. Cluster 1 has a fatter letter, cluster 2 is more formal and centered, and cluster 3 leans a little to the right.\n\n\nCompare models\n\n\n\n\nAccuracy of models\n \n  \n    model \n    accuracy \n  \n \n\n  \n    kNN \n    0.96 \n  \n  \n    Support vector classifier \n    0.98 \n  \n  \n    Random forest \n    0.97 \n  \n\n\n\n\n\nWe could see that the accuracy of the Support vector classifier model is the best, which is 0.98.\n\n\nConclusion\nThe kNN model, the Support vector classifier model with the Polynomial kernel and the Random forest model are both good for classifying the new record of 5 observations, but following the accuracy of each model, I recommend the Support vector classifier model with the Polynomial kernel."
  },
  {
    "objectID": "posts/Machine Learning/assignment3-template.html",
    "href": "posts/Machine Learning/assignment3-template.html",
    "title": "ETC3250/5250 IML Asignment 3 Solution",
    "section": "",
    "text": "# Load the packages that you will use to complete this assigment.\n\nlibrary(tidyverse)\nlibrary(kableExtra)\nlibrary(patchwork)\nlibrary(kknn)\nlibrary(yardstick)\nlibrary(rsample)\nlibrary(e1071)\nlibrary(ranger)\n\nmydata <- read.csv(\"data32649479.csv\") \nnewrecord <- read.csv(\"newrecords32649479.csv\")\n\nset.seed(32649479)"
  },
  {
    "objectID": "posts/Machine Learning/assignment3-template.html#preliminary-analysis",
    "href": "posts/Machine Learning/assignment3-template.html#preliminary-analysis",
    "title": "ETC3250/5250 IML Asignment 3 Solution",
    "section": "Preliminary analysis",
    "text": "Preliminary analysis\n\nQuestion 1\nThe letter in my data is ‘y’.\n\n\n\nQuestion 2\n\nimagedata_to_plotdata <- function(data = mydata, \n                                  w = 28, \n                                  h = 28, \n                                  which = sample(1:3448, 12)) {\n  data %>% \n    mutate(id = 1:n()) %>% \n    filter(id %in% which) %>% \n    pivot_longer(starts_with(\"V\")) %>% \n    mutate(col = rep(rep(1:w, each = h), n_distinct(id)),\n           row = rep(rep(1:h, times = w), n_distinct(id)))\n}\n\ngletter <- imagedata_to_plotdata(mydata) %>% \n    ggplot(aes(col, row)) +\n    geom_tile(aes(fill = value)) + \n    facet_wrap(~id, nrow = 3) +\n    scale_y_reverse() +\n    theme_void(base_size = 18) +\n    guides(fill = \"none\") +\n    coord_equal()\ngletter\n\n\n\n\n\n\n\nQuestion 3\n\nmydata_pca_5 <- prcomp(mydata, rank. = 5)\nsummary(mydata_pca_5)\n\nImportance of first k=5 (out of 784) components:\n                            PC1      PC2       PC3       PC4       PC5\nStandard deviation     667.3433 558.4448 520.02544 418.33520 408.85848\nProportion of Variance   0.1435   0.1005   0.08712   0.05638   0.05385\nCumulative Proportion    0.1435   0.2439   0.33105   0.38742   0.44128\n\n\nWe can see that the first five principal components explain 44.128% of the variation in the data, and PC1, PC2, PC3, PC4 and PC5 explain 14.35%, 10.05%, 8.712%, 5.638% and 5.385% of the variation in the data respectively.\n\n\nQuestion 4\n\nmydata_pca <- prcomp(mydata)\n\npc_decompose <- function(k) {\n    Xnew <- mydata_pca$x[, k, drop = FALSE] %*% t(mydata_pca$rotation[, k, drop = FALSE])\n  rawdata <- imagedata_to_plotdata(which = sample(1:3448, 1)) \n  as.data.frame(Xnew) %>% \n    mutate(id = mydata_pca$id) %>% \n    imagedata_to_plotdata(which = sample(1:3448, 1))\n}\n\nset.seed(32649479)\ngletter %+% pc_decompose(1) + labs(title = \"PC1 loading\")\n\n\n\nset.seed(32649479)\ngletter %+% pc_decompose(2) + labs(title = \"PC2 loading\")\n\n\n\n\n\n\nQuestion 5\n\nhaverage <- hclust(dist(mydata_pca$x), method = \"average\")\nhaverage\n\n\nCall:\nhclust(d = dist(mydata_pca$x), method = \"average\")\n\nCluster method   : average \nDistance         : euclidean \nNumber of objects: 3448 \n\n\n\n\n\nQuestion 6\n\ncave <- cutree(haverage, k = 4)\ntable(cave) %>%\n  kable(caption = \"How many observations in the 4 clusters.\") %>%\n  kable_styling()\n\n\n\nHow many observations in the 4 clusters.\n \n  \n    cave \n    Freq \n  \n \n\n  \n    1 \n    3441 \n  \n  \n    2 \n    4 \n  \n  \n    3 \n    2 \n  \n  \n    4 \n    1 \n  \n\n\n\n\n\n\n\nQuestion 7\n\nview_cluster <- function(k) {\n  cluster <- mydata %>% \n    mutate(group = cave) %>% \n    filter(group == k) %>% \n    imagedata_to_plotdata(which = 1:10)\n  \n  gletter %+% cluster\n}\n\np1 <- view_cluster(1) +labs(title = \"Cluster 1\")+\n  facet_wrap(~id, nrow = 5)\n\np2 <- view_cluster(2) + labs(title = \"Cluster 2\") +\n  facet_wrap(~id, nrow = 2)\n  \np3 <- view_cluster(3) + labs(title = \"Cluster 3\") +\n  facet_wrap(~id, nrow = 1)\n\np4 <- view_cluster(4) + labs(title = \"Cluster 4\") +\n  facet_wrap(~id, nrow = 1)\n\np <- p1+(p2/p3/p4)\np + plot_annotation(title = \"A sample of images from each cluster\")\n\n\n\n\nWe could see that the tail of the letter ‘y’ seems to be straight and the upper part more like ‘v’ in cluster 1, while the tail of the letter ‘y’ is more curved and the upper part of the letter ‘y’ is more like ‘u’ in cluster 2. Then in cluster 3, the upper part of the letter ‘y’ is narrower in width than the lower part, and finally, in cluster 4 the upper part of the letter ‘y’ is like ‘u’ and the lower part of the letter ‘y’ is like ‘v’."
  },
  {
    "objectID": "posts/Machine Learning/assignment3-template.html#report",
    "href": "posts/Machine Learning/assignment3-template.html#report",
    "title": "ETC3250/5250 IML Asignment 3 Solution",
    "section": "Report",
    "text": "Report\n\nChoose pricipal components\nTo explore the way each person writes letters and to categorize the way they write them. I do a dimension reduction to the pixel data of a letter. And then, I produce a scree plot, and the elbow appears around the fourth to seventh PC (Principal component), therefore I choose 5 PCs (Principal components) to be used to analyze, and the 5 PCs (Principal components) explain 44.12% of the variation in the data.\n\ntibble(var = mydata_pca$sdev^2) %>%\n  mutate(PC = factor(1:n())) %>%\n  mutate(PC = as.numeric(PC)) %>%\n  filter(PC <= 15) %>%\n  mutate(PC = factor(PC)) %>%\n  ggplot(aes(PC, var)) +\n  geom_point() +\n  geom_line(group = 1)+\n  labs(title = \"Scree plot\")+\n  theme_bw()\n\n\n\nset.seed(32649479)\nmydata_pca_5 <- prcomp(mydata, rank. = 5)\nsummary(mydata_pca_5)\n\nImportance of first k=5 (out of 784) components:\n                            PC1      PC2       PC3       PC4       PC5\nStandard deviation     667.3433 558.4448 520.02544 418.33520 408.85848\nProportion of Variance   0.1435   0.1005   0.08712   0.05638   0.05385\nCumulative Proportion    0.1435   0.2439   0.33105   0.38742   0.44128\n\n\n\n\n\nClassify data\nI choose the k-means method to divide the different ways that a person can write a particular letter into 3 clusters. This plot shows that the letter ‘y’ is more centered in cluster 1, and the letter ‘y’ is italic, the letter tends to lean to the right in cluster 2, and the letter ‘y’ is fatter and the upper part of the letter ‘y’ resembles a ‘u’ in cluster 3.\n\nset.seed(32649479)\nkout <- kmeans(mydata_pca_5$x, centers = 3)\n\nview_cluster_report <- function(k) {\n  cluster <- mydata %>% \n    mutate(group = kout$cluster) %>% \n    filter(group == k) %>% \n    imagedata_to_plotdata(which = 1:20)\n  \n  gletter %+% cluster\n}\n\np1 <- view_cluster_report(1) + labs(title = \"Cluster 1\")+\n  facet_wrap(~id, nrow = 2)\n\np2 <- view_cluster_report(2) + labs(title = \"Cluster 2\")+\n  facet_wrap(~id, nrow = 2)\n\np3 <- view_cluster_report(3) + labs(title = \"Cluster 3\")+\n  facet_wrap(~id, nrow = 2)\n\np <- p1/p2/p3\np + plot_annotation(title = \"Different ways people write a letter\")\n\n\n\n\n\nimagedata_to_newrecord <- function(data = newrecord, \n                                  w = 28, \n                                  h = 28, \n                                  which = sample(1:5, 5)) {\n  data %>% \n    mutate(id = 1:n()) %>% \n    filter(id %in% which) %>% \n    pivot_longer(starts_with(\"V\")) %>% \n    mutate(col = rep(rep(1:w, each = h), n_distinct(id)),\n           row = rep(rep(1:h, times = w), n_distinct(id)))\n}\n\nnew_gletter <- imagedata_to_newrecord(newrecord) %>% \n    ggplot(aes(col, row)) +\n    geom_tile(aes(fill = value)) + \n    facet_wrap(~id, nrow = 3) +\n    scale_y_reverse() +\n    theme_void(base_size = 18) +\n    guides(fill = \"none\") +\n    coord_equal()\n\n\n\nClassify new records\nFor classifying a set of new record of 5 observations, I use 3 supervised learning methods, and I set principal components as predictors.\n\nset.seed(32649479)\nx <- mydata_pca_5$x\ncluster <- as_tibble(x) %>% \n    mutate(group = factor(kout$cluster))# create mydata's response variable, PCs as predictors.\n\nnewrecord_pca <- prcomp(newrecord)\nnewdata <- as_tibble(newrecord_pca$x) #making PCs as predictors in newdata.\n\nI create three models by using the kNN method, the Support vector classifier method with Polynomial kernel and the Random forest method respectively to classify the new 5 observations.\n\nkNN method\n\n\nset.seed(32649479)\nknn_pred <- kknn(group~.,\n                 train = cluster,\n                 test = newdata,\n                 k = 15,\n                 distance = 2) #knn\n\nview_cluster_report <- function(k) {\n  cluster <- newrecord %>% \n    mutate(group = knn_pred$fitted.values) %>% \n    filter(group == k) %>% \n    imagedata_to_newrecord(which = 1:5)\n  \n  new_gletter %+% cluster\n}\n\np1 <- view_cluster_report(1) + labs(title = \"Cluster 1\")+\n  facet_wrap(~id, nrow = 1)\n\np2 <- view_cluster_report(2) + labs(title = \"Cluster 2\")+\n  facet_wrap(~id, nrow = 1)\n\np3 <- view_cluster_report(3) + labs(title = \"Cluster 3\")+\n  facet_wrap(~id, nrow = 1)\n\npk <- p1/p2/p3\npk + plot_annotation(title = \"kNN model\") \n\n\n\n\n\nSupport vector classifier method with Polynomial kernel\n\n\nset.seed(32649479)\npoly_svm <- svm(group ~ ., data = cluster, cost =1, kernel = \"polynomial\") #svm\npred_poly <- predict(poly_svm, newdata = newdata)\n\nview_cluster_report <- function(k) {\n  cluster <- newrecord %>% \n    mutate(group = pred_poly) %>% \n    filter(group == k) %>% \n    imagedata_to_newrecord(which = 1:5)\n  \n  new_gletter %+% cluster\n}\n\np1 <- view_cluster_report(1) + labs(title = \"Cluster 1\")+\n  facet_wrap(~id, nrow = 1)\n\np2 <- view_cluster_report(2) + labs(title = \"Cluster 2\")+\n  facet_wrap(~id, nrow = 1)\n\np3 <- view_cluster_report(3) + labs(title = \"Cluster 3\")+\n  facet_wrap(~id, nrow = 1)\n\npsvm <- p1/p2/p3\npsvm + plot_annotation(title = \"Support vector classifier model\")\n\n\n\n\n\nRandom forest method\n\n\nset.seed(32649479)\nclass_rf <- ranger(group~., \n                   data = cluster,\n                   importance = \"impurity\",\n                   classification = TRUE)\n\npredict<- newdata %>%\n  mutate(group = predict(class_rf, .)$predictions)\n\nview_cluster_report <- function(k) {\n  cluster <- newrecord %>% \n    mutate(group = predict$group) %>% \n    filter(group == k) %>% \n    imagedata_to_newrecord(which = 1:5)\n  \n  new_gletter %+% cluster\n}\n\np1 <- view_cluster_report(1) + labs(title = \"Cluster 1\")+\n  facet_wrap(~id, nrow = 1)\n\np2 <- view_cluster_report(2) + labs(title = \"Cluster 2\")+\n  facet_wrap(~id, nrow = 1)\n\np3 <- view_cluster_report(3) + labs(title = \"Cluster 3\")+\n  facet_wrap(~id, nrow = 1)\n\nprf <- p1/p2/p3\nprf + plot_annotation(title = \"Random forest model\")\n\n\n\n\nWe can see that the classified results following the three models are the same and sensible, we could easily see that these are different 3 groups. Cluster 1 has a fatter letter, cluster 2 is more formal and centered, and cluster 3 leans a little to the right.\n\n\nCompare models\n\nset.seed(32649479)\n\ncluster_split <- initial_split(cluster)\ncluster_train <- training(cluster_split)\ncluster_test <- testing(cluster_split)\nknn_check <- kknn(group~.,\n                 train = cluster_train,\n                 test = cluster_test,\n                 k = 15,\n                 distance = 2) #knn\nknn_accuracy <- accuracy_vec(cluster_test$group, as.factor(knn_check$fitted.values))\n\n\npoly_svm_check <- svm(group ~ ., data = cluster_train, cost =1, kernel = \"polynomial\") #svm \npred_poly_check <- predict(poly_svm_check, newdata = cluster_test)\npoly_svm_accuracy <- accuracy_vec(cluster_test$group, as.factor(pred_poly_check))\n\n\nclass_rf_check <- ranger(group~., \n                   data = cluster_train,\n                   importance = \"impurity\",\n                   classification = TRUE) #rf\n\npredict<- cluster_test %>%\n  mutate(group = predict(class_rf_check, .)$predictions)\n\nrf_accuracy <- accuracy_vec(cluster_test$group, as.factor(predict$group))\n\ndata_frame(model = c(\"kNN\", \"Support vector classifier\", \"Random forest\"),\n                   accuracy = c(round(knn_accuracy,2), round(poly_svm_accuracy,2), round(rf_accuracy,2))) %>%\n  kable(caption = \"Accuracy of models\") %>%\n  kable_styling()\n\n\n\nAccuracy of models\n \n  \n    model \n    accuracy \n  \n \n\n  \n    kNN \n    0.96 \n  \n  \n    Support vector classifier \n    0.98 \n  \n  \n    Random forest \n    0.97 \n  \n\n\n\n\n\nWe could see that the accuracy of the Support vector classifier model is the best, which is 0.98.\n\n\nConclusion\nThe kNN model, the Support vector classifier model with the Polynomial kernel and the Random forest model are both good for classifying the new record of 5 observations, but following the accuracy of each model, I recommend the Support vector classifier model with the Polynomial kernel."
  },
  {
    "objectID": "posts/Elec Fuel/index.html#introduction-and-motivation",
    "href": "posts/Elec Fuel/index.html#introduction-and-motivation",
    "title": "Alternative Fuel Stations",
    "section": "Introduction and motivation",
    "text": "Introduction and motivation\nIn the advent of 21st century, automotive industry and its energy industry, particularly fuel, are increasing in popularity drastically and rapidly. However, most have rarely give significant thoughts about their sustainability of using fuel which are made from fossils, let alone its massive footprint of Carbon Dioxide and other humanly and environmentally threatening substances emerged from internal combustion engine.\nFortunately, many countries have raise awareness and concern about this subject and heavily push for a change - especially in alternative fuel usage. In which led us into this report’s topic of “Alternative Fuel Stations”. Referring to Figure1, we can see that alternative fuel used to be kick-started since the late 1990s - according to US Department of Energy, this was Methanol usage instead of gasoline with the peak usage of almost 6 million gallon equivalents of gasoline used in that decade period. Nevertheless, its popularity was short-lived and majority of US citizens switched back to pure fossil fuel with its all time low alternative usage in 2007.\nNonetheless, referring back again to Figure1, after the 2008 crisis, it kick-started the second coming of alternative fuel which shows more sign of sustainability and gradual growth with a healthy uptrend onward from 2010 up to present. This promising trend illustrate that substantial amount of people are caring more and more toward alternative fuels in which affects directly to fuel stations need of installing more alternative fuel ports and emerging of new solely alternative fuel-based station.\nHenceforth, this report aims to focus about alternative fuel stations all across the United States - focusing on problems between states specifically on trends relative to alternative fuel stations.\n\n\n\n\n\nFigure1: The trend of ports number and elec stations number. Note: The y-axis is log transformed."
  },
  {
    "objectID": "posts/Elec Fuel/index.html#electric-stations-and-port-number-1995-2021",
    "href": "posts/Elec Fuel/index.html#electric-stations-and-port-number-1995-2021",
    "title": "Alternative Fuel Stations",
    "section": "Electric stations and port number (1995-2021)",
    "text": "Electric stations and port number (1995-2021)\n\n\n\n\n\n\n\n\n\n\nMap1: Map of electric stations in US during 1995-2021\n\n\nFrom Map1, we can see the exact location and name of the electric station and the number of ports it has by zooming in on this map.\n\nHow does the number of electric stations change by state over year?\n\n\n\n\n\nFigure2: The trend of electric stations over year by state. Note: The y-axis is log transformed.\n\n\n\n\n\n\n\n\n\nFigure3: The trend of electric stations over year in the four specimen states. Note: The y-axis is log transformed.\n\n\n\n\nTo analyze this question, we calculate the sum of electric stations which have opened before 2022 by state over year. See Figure2, only California (CA) opened electric stations before 2007, and while most states started to open electric stations after 2008. Furthermore, they are also leading state to install alternative fuel stations, potentially was Methanol fuel ports as we have discussed in the introduction part. Interestingly, during that period of 1990s, not a single state have these Methanol fuel stations installed at all.\nFrom this point of view we can imply that California is highly innovative and experimental, especially it can be further perceived that California state’s citizen are extremely environmentalist and Eco-friendly mindset.\nFurthermore, for more meaningful analysis, states of California (CA), New York (NY), Alaska (AK), North Dakota (ND), Ontario (ON), and Puerto Rico (PR) are selected as specimens to analyse in a comparative and contrasting methodology. See Figure3, CA and NY started to open electric stations in 1997 and in 2008 respectively, and the number of electric stations in CA and NY keeps growing and surpasses 100 stations rapidly and sustain-ably. These two states are particularly known to be a “big” state with major capitals, hence in some ways it is expected to see CA and NY to be big early mover for innovations for Eco-friendly and sustainability technology.\nAccording to CBS, New York has been pushing for electric stations and charging ports heavily recently. As many people are starting to purchase more of electric car due to the fact that it made them feel good about the reduction of emissions which is significantly lower than relying on gasoline and internal combustion engine. However, the reason that California has it leads to large margin in terms of electric stations installed is because of rules and regulations of mandating the ownership of electric car, along with multiple incentive, namely price discount and tax refund. Hence these reasons are truly justified why these two states are having a great extensive number of electric stations, and both California and New York states should be ideal examples for other states to follow.\nOn the other hand, AK and ND, both of which are supposedly smaller states in the USA, are undoubtedly having less number of electric stations and open most recently as 2015 and 2011 respectively. It is understandable that perhaps electric vehicles’ accessibility for people in these two states could be much lower. Nevertheless, both AK and ND should be admirable for having stable number of electric stations open in the last 4 years.\nOn the other extreme ends, ON and PR have just started to open electric station in 2021 with very few number of stations as well. Even though they are territories in Canada and presumably smaller states in comparison to other states, they still have to follow other states and lift increasing in awareness and promotion of electric vehicles.\nOverall, the United States and Canadian territories are in a healthy trend of opening new alternative fuel stations with more and more encouragement and awareness toward larger group of people.\n\n\nDoes the number of electric stations have a effect on the number of ports ?\n\n\n\n\n\nFigure4: The relationship between ports and ELEC stations over year. Note: The y-axis is log transformed.\n\n\n\n\n\n\n\n\n\nFigure5: The trend of proporation(ports/stations) over year by state. Note: The y-axis is log transformed.\n\n\n\n\n\n\n\n\n\nFigure6: The trend of proporation(ports/stations) over year in the four states. Note: The y-axis is log transformed.\n\n\n\n\nTo see the relationship between the number of electric ports and the number of electric stations, we calculated the ports number per station provides over year as Figure4, in which it shows that proportion of ports number and stations number shows a downtrend over year overall, then the number is stabilized after 2012 in a slightly downward trend within the range of mere 1 station.\nHowever, the key takeaway from this plot is trend of number of ports which is in a decreasing pattern instead of increasing, with the growth of number of electric stations. We do found this very counter-intuitive, surprising yet interesting.\nHence the reason why we want to further focus on the proportion in each state. See Figure5, the proportion in most of states is in the range of 2 and 3 with a more stabilized trend to slightly downward trend.\nReferring to Figure6, we have chosen interesting states with extremely different value for the sake of clear comparison, namely California (CA), Ontario (ON), Utah (UT), Wyoming (WY). The proportion of CA reached the peak which is around 30 during 1999, and the proportion of ON, UT, WY states has value under 1, which means there are some electric stations cannot even provide one port and even uninstall in during the particular year.\nOur preliminary assumption is that electric charging ports are becoming more privatized and stray away from traditional fuel stations. For example, in the case of Tesla motor which encourage to cut the middle man both vehicle sales and charging ports station - both of which are funded by the company. All of this is to make electric vehicles more accessible and charging ports more convenient and accessible for majority of their customer and fellow electric car user alike.\nAccording to Hawkins, these chargers are becoming more and more accessible and thoroughly installed with more option of charger like “Supercharger” and compatibility beyond Tesla vehicle only. Hence, these non-fuel-station-tied charger proved to be more versatile and flexible in comparison to traditional ports available at fuel station. By seeing this trend, it proved to be sensible and understandable that this could be potential new trends for the near future."
  },
  {
    "objectID": "posts/Elec Fuel/index.html#conclusion",
    "href": "posts/Elec Fuel/index.html#conclusion",
    "title": "Alternative Fuel Stations",
    "section": "Conclusion",
    "text": "Conclusion\nIn conclusion, we believed that the future of electric vehicles and ports in alternative fuel stations are as bright as ever. With significant number of states are increasing in stations with charger, better EVSE ports are constantly upgrading and even motor companies are contributing in encouraging and installation, electric vehicle could be a new norm within two decades."
  },
  {
    "objectID": "posts/Elec Fuel/index.html#reference",
    "href": "posts/Elec Fuel/index.html#reference",
    "title": "Alternative Fuel Stations",
    "section": "Reference",
    "text": "Reference\n[1]Bureau of Transportation Statistics. https://data-usdot.opendata.arcgis.com/datasets/usdot::alternative-fueling-stations/about.\n[2]Alternativa Fuels Data Center.  https://afdc.energy.gov/fuels/emerging_methanol.html#:~:text=Methanol%20was% 20marketed%20in%20the,vehicles%20in%20the%20United%20States.\nData reference\nTransportation Statistics, B of (2022). Alternative Fueling Stations. U.S. Department of Transportation. https://data-usdot.opendata.arcgis.com/datasets/usdot::alternative-fueling-stations/about.\nPackages and citation reference\nAlternative Fuels Data Center: Methanol. (n.d.). Afdc.energy.gov. Retrieved August 26, 2022, from https://afdc.energy.gov/fuels/emerging_methanol.html#:~:text=Methanol%20was%20marketed%20in% 20the.\nAuguie, B (2017). gridExtra: Miscellaneous Functions for “Grid” Graphics. R package version 2.3.https://CRAN.R-project.org/package=gridExtra.\nCharger map - Electric Vehicle Council. (n.d.). https://electricvehiclecouncil.com.au/about-ev/charger-map/.\nGrolemund, G and H Wickham (2011). Dates and Times Made Easy with lubridate. Journal of Statistical Software 40(3), 1–25.\nHarrell Jr, FE (2022). Hmisc: Harrell Miscellaneous. R package version 4.7-1. https://CRAN.R-project.org/package=Hmisc.\nHawkins, A. J. (2022, July 7). Tesla will open up Superchargers to non-Tesla electric vehicles in the US later this year. The Verge. https://www.theverge.com/2022/7/7/23198696/tesla-supercharger-non-tesla-ev-us-white-house.\nMore electric vehicle charging stations open in New York City as more drivers steer away from gasoline. (n.d.). Www.cbsnews.com. Retrieved August 26, 2022, from https://www.cbsnews.com/newyork/news/electric-vehicle-charging-stations-new-york-city-con-edison/.\nTierney,N,DCook,MMcBain,andCFay (2021). naniar : DataStructures, Summaries, andVisualisations for Missing Data. R package version 0.6.1. https://CRAN.R-project.org/package=naniar.\nWickham, H, M Averick, J Bryan, W Chang, LD McGowan, R François, G Grolemund, A Hayes, L Henry, J Hester, M Kuhn, TL Pedersen, E Miller, SM Bache, K Müller, J Ooms, D Robinson, DP Seidel, V Spinu, K Takahashi, D Vaughan, C Wilke, K Woo, and H Yutani (2019). Welcome to the tidyverse. Journal of Open Source Software 4(43), 1686.\nWickham, H and J Bryan (2022). readxl: Read Excel Files. R package version 1.4.0. https://CRAN.R-project.org/package=readxl.\nXie, Y (2022). knitr: A General-Purpose Package for Dynamic Report Generation in R. R package version1.38. https://yihui.org/knitr/.\nYihui Xie (2022). bookdown: Authoring Books and Technical Documents with R Markdown. R package version 0.28.\nZhu, H (2021). kableExtra: Construct Complex Table with ’kable’ and Pipe Syntax. R package version1.3.4. https://CRAN.R-project.org/package=kableExtra."
  },
  {
    "objectID": "posts/Welcome/index.html",
    "href": "posts/Welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "Greetings! I’m excited to welcome you to my blog, a platform dedicated to showcasing the outcomes of my graduate journey in the field of Business Analytics. As a passionate learner in this domain, I’m thrilled to share the fruits of my studies with you.\nThis digital space encapsulates a collection of my endeavors, primarily focused on the realms of machine learning, exploratory data analysis, communication with data and forecasting.\nThis blog serves as a testament to my growth as a data enthusiast. The process of transforming raw data into compelling visualizations and compelling stories has been a journey of creativity and technical acumen.\nThrough this blog, I aim to share my insights and techniques, contributing to the broader discourse on the importance of clear and impactful data communication.\nJoin me on this exciting journey as we unravel the power of data. Thank you for being a part of this exploration!"
  },
  {
    "objectID": "posts/The Gini index across US  states analysis/index.html",
    "href": "posts/The Gini index across US  states analysis/index.html",
    "title": "The Gini index across US states analysis",
    "section": "",
    "text": "The Gini coefficient offers a quantitative lens to assess income inequality, a pressing societal concern. It scales from zero, representing complete equality, to one, denoting extreme inequality, thus encapsulating the economic disparities within a society (Adam, 2023).\nThis report delves into the variations in the Gini index across US states. At its core, we’re interested in examining how pivotal events such as the Great Depression, World War II, and the Global Financial Crisis influenced state-level income disparities.\nTo ensure rigorous insights, we begin with detailed data preparation and cleansing. This foundational step paves the way for insightful data visualizations, highlighting intrinsic trends and anomalies.\nWe employ Principal Component Analysis (PCA) and Multidimensional Scaling (MDS) as our primary analytical tools, recognized for their prowess in reducing multidimenstional data. The report will also touch upon the potential constraints and issues faced throughout our investigation.”"
  },
  {
    "objectID": "posts/The Gini index across US  states analysis/index.html#datasets-overview",
    "href": "posts/The Gini index across US  states analysis/index.html#datasets-overview",
    "title": "The Gini index across US states analysis",
    "section": "Datasets Overview",
    "text": "Datasets Overview\nThe primary dataset for this analysis has been procured from the repository titled “U.S. State-Level Income Inequality Data”, diligently compiled by Mark W. Frank. The data repository furnished two distinct datasets for the periods: 1929 to 1945: Capturing the Gini index values across 50 U.S. states. 2007 to 2015: This dataset details the Gini index values for an expanded list of 52 U.S. states, reflecting the statehood transitions post-1959, Alaska and Hawaii became states. Given the absence of data from 1946 to 2006, and the changes in statehood, it was decided not to merge the two datasets."
  },
  {
    "objectID": "posts/The Gini index across US  states analysis/index.html#data-cleaning-and-imputation",
    "href": "posts/The Gini index across US  states analysis/index.html#data-cleaning-and-imputation",
    "title": "The Gini index across US states analysis",
    "section": "Data Cleaning and Imputation:",
    "text": "Data Cleaning and Imputation:\n\n\nCode\nInequality_GD <- read_csv(\"Inequality_GD.csv\") %>% \n            select(-c(\"...1\"))\nInequality_GR <- read_csv(\"Inequality_GR.csv\")%>% \n             select(-c(\"...1\"))\n\n\n\nKNN imputation for missing value\nThe dataset from 2007 to 2015 exhibited missing values for some states in 2010, accounting for approximately 1.5% Figure 1 of the entire dataset. To address this, we applied KNN imputation, which takes into account inter-variable relationships. This method not only preserves a more natural variability in imputed values but also offers a more sophisticated approach than mere column-wise mean imputation.\n\n\nCode\nInequality_GR %>% vis_miss(warn_large_data = FALSE)\n\n\n\n\n\nFigure 1: Percentage of missing value between 2007 and 2015\n\n\n\n\n\n\nMean imputation for outlier\nFrom the Figure 2, an evident outlier was identified in the 1929 to 1945 dataset: Oregon’s Gini index recorded as 1000 in 1934. Considering the Gini index’s range is between 0 and 1, this is a data entry error. We employed mean imputation to address this, calculating the average from the years immediately preceding and following 1934 for Oregon.\n\n\nCode\nInequality_GD_long <- Inequality_GD %>%\n  pivot_longer(cols = `1929`:`1945`, \n               names_to = \"year\", \n               values_to = \"gini index\")\n\n\nInequality_GR_long <- Inequality_GR %>%\n  pivot_longer(cols = `2007`:`2015`, \n               names_to = \"year\", \n               values_to = \"gini index\")\n\nggplot(Inequality_GD, aes(x = State, y = `1934`)) + \n    geom_point() +\n    theme(axis.text.x = element_text(angle = 45, hjust = 1)) +\n    ggtitle(\"Gini Index for US States in 1934\")\n\n\n\n\n\nFigure 2: The outlier of Gini index in 1934\n\n\n\n\n\n\nCode\n##KNN imputation \nstate_col <- Inequality_GR$State\n\n# Remove 'State' column for imputation\nGR_for_imputation <- Inequality_GR[, -which(names(Inequality_GR) == \"State\")]\n\nmin_max_scale <- function(x) {\n    (x - min(x, na.rm = TRUE)) / (max(x, na.rm = TRUE) - min(x, na.rm = TRUE))\n}\n\nGR_normalized <- as.data.frame(lapply(GR_for_imputation, min_max_scale))\n\nGR_imputed <- knnImputation(GR_normalized, k = 5)\n\n\noriginal_mins <- apply(GR_for_imputation, 2, min, na.rm = TRUE)\noriginal_maxs <- apply(GR_for_imputation, 2, max, na.rm = TRUE)\n\nGR_rescaled <- as.data.frame(mapply(function(col, min_val, max_val) {\n    col * (max_val - min_val) + min_val\n}, GR_imputed, original_mins, original_maxs, SIMPLIFY=FALSE))\n\nnames(GR_rescaled) <- names(GR_for_imputation)\n\nGR_rescaled$State <- state_col\n\n\n\n\nCode\noregon_1933 <- Inequality_GD[Inequality_GD$State == \"Oregon\", \"1933\"]\noregon_1935 <- Inequality_GD[Inequality_GD$State == \"Oregon\", \"1935\"]\n\n# Replace the 1934 value with the average of 1933 and 1935 for Oregon\nInequality_GD[Inequality_GD$State == \"Oregon\", \"1934\"] <- \n  (oregon_1933 + oregon_1935) / 2 \n\nGD_new <-Inequality_GD"
  },
  {
    "objectID": "posts/The Gini index across US  states analysis/index.html#other-data-credibility-issues",
    "href": "posts/The Gini index across US  states analysis/index.html#other-data-credibility-issues",
    "title": "The Gini index across US states analysis",
    "section": "Other data credibility issues",
    "text": "Other data credibility issues\nWe ensured data consistency and corrected spatial discrepancies for accurate results. For better visualization, adding column to provides state abbreviations, like “Los Angeles” as “LA”."
  },
  {
    "objectID": "posts/The Gini index across US  states analysis/index.html#classic-mds",
    "href": "posts/The Gini index across US  states analysis/index.html#classic-mds",
    "title": "The Gini index across US states analysis",
    "section": "Classic mds",
    "text": "Classic mds\n\n\nCode\nGD_sf <- left_join(GD_new,GD_states_sf,by = c(\"State\" = \"NAME\")) %>%\n  select(-State) %>%\n  rename(\"State\" = \"abb\")\n\nGD_sf$State[is.na(GD_sf$State)] <- \"US\"\n\n\nGR_sf <- left_join(GR_rescaled,GR_states_sf,by = c(\"State\" = \"NAME\")) %>%\n  select(-State) %>%\n  rename(\"State\" = \"abb\")\n\nGR_sf$State[is.na(GR_sf$State)] <- \"US\"\n\n\n\n\nCode\nmds <- GD_new %>%\n  select(-1) %>% \n  dist() %>%          \n  cmdscale() %>%\n  as_tibble() \n\ncolnames(mds) <- c(\"Dim.1\", \"Dim.2\")\n\n# K-means clustering\nclust <- kmeans(mds, 4)$cluster %>%\n  as.factor()\nmds <- mds %>%\n  mutate(groups = clust,\n         state = GD_sf$State)\n# Plot and color by groups\nggscatter(mds, x = \"Dim.1\", y = \"Dim.2\", \n          label = \"state\",\n          color = \"groups\",\n          palette = \"jco\",\n          size = 1, \n          ellipse = TRUE,\n          ellipse.type = \"convex\",\n          repel = TRUE) +\n  ggtitle(\"Classical MDS 1929-1945\") -> mds1\n\n\n\n\nCode\nmds <- GR_rescaled %>%\n  select(-State) %>% \n  dist() %>%          \n  cmdscale() %>%\n  as_tibble() \n\ncolnames(mds) <- c(\"Dim.1\", \"Dim.2\")\n\n# K-means clustering\nclust <- kmeans(mds, 4)$cluster %>%\n  as.factor()\nmds <- mds %>%\n  mutate(groups = clust,\n         state = GR_sf$State)\n# Plot and color by groups\nggscatter(mds, x = \"Dim.1\", y = \"Dim.2\", \n          label = \"state\",\n          color = \"groups\",\n          palette = \"jco\",\n          size = 1, \n          ellipse = TRUE,\n          ellipse.type = \"convex\",\n          repel = TRUE)+\n  ggtitle(\"Classical MDS 2007-2015\") -> mds2\n\n\n\n\nCode\nmds1 + mds2\n\n\n\n\n\nFigure 5: MDS analysis Plot\n\n\n\n\nFrom the Figure 5, similarities and dissimilarities are evident based on distances between data points. For instance, in the “Classical MDS 1929-1945” plot, Delaware appears as a potential outlier. Yet, in the “Classical MDS 2007-2015”, Delaware clusters with states like West Virginia and Washington, while California and others emerge as outliers."
  },
  {
    "objectID": "posts/The Gini index across US  states analysis/index.html#sammon-mapping",
    "href": "posts/The Gini index across US  states analysis/index.html#sammon-mapping",
    "title": "The Gini index across US states analysis",
    "section": "Sammon Mapping",
    "text": "Sammon Mapping\nWe use another type of MDS that is called sammon mapping to see whether our conclusion still holds. Sammon mapping is not based on eigenvalue decomposition and linear mapping so that it preserves the local structure.\n\n\nCode\n# GD 2007-2015\nsam <- GR_rescaled %>%\n  select(-1) %>% \n  dist() %>%          \n  sammon()\n\n\nInitial stress        : 0.00145\nstress after  10 iters: 0.00096, magic = 0.500\nstress after  20 iters: 0.00096, magic = 0.500\n\n\nCode\nsam$points %>% \n  as_tibble() -> sam\n\ncolnames(sam) <- c(\"Sam.1\", \"Sam.2\")\n\n# K-means clustering\nclust <- kmeans(sam, 4)$cluster %>%\n  as.factor()\n\nsam <- sam %>%\n  mutate(groups = clust,\n         state = GR_sf$State)\n# plot\nggscatter(sam, x = \"Sam.1\", y = \"Sam.2\", \n          label = \"state\",\n          color = \"groups\",\n          palette = \"jco\",\n          size = 1, \n          ellipse = TRUE,\n          ellipse.type = \"convex\",\n          repel = TRUE)+\n  ggtitle(\"SAM 2007-2015\") -> Sam2\n\n\n\n\nCode\nSam1 + Sam2\n\n\n\n\n\nFigure 6: Sammon mapping analysis Plot\n\n\n\n\nAs you can see from the Figure 6, it has a similar pattern to classic MDS, but some states, such as North Dakota and New Mexico will be further away (not similar)."
  },
  {
    "objectID": "posts/The Gini index across US  states analysis/index.html#great-depression-and-world-war-ii",
    "href": "posts/The Gini index across US  states analysis/index.html#great-depression-and-world-war-ii",
    "title": "The Gini index across US states analysis",
    "section": "Great Depression and World War II",
    "text": "Great Depression and World War II\n\nImportance of PCs(pricipal components)\nFrom our analysis, PC1 accounts for 75.8% of the total variance, while PC2 contributes an additional 15.28%. Together, PC1 and PC2 capture 91.08% of the data’s overall variance. This high cumulative proportion suggests that these two components predominantly encapsulate the dataset’s information, validating our choice for dimensionality reduction.\n\n\nCode\n#########PCA ######\nGD_pca<- GD_sf %>% \n  column_to_rownames('State') %>% \n  prcomp(scale. = TRUE)\n\n summary(GD_pca)\n\n\nImportance of components:\n                         PC1    PC2     PC3     PC4     PC5     PC6     PC7\nStandard deviation     3.590 1.6116 0.78748 0.57044 0.35623 0.34949 0.29707\nProportion of Variance 0.758 0.1528 0.03648 0.01914 0.00746 0.00718 0.00519\nCumulative Proportion  0.758 0.9108 0.94730 0.96644 0.97391 0.98109 0.98628\n                           PC8     PC9    PC10    PC11    PC12    PC13    PC14\nStandard deviation     0.24100 0.21648 0.20062 0.17286 0.12792 0.12232 0.11231\nProportion of Variance 0.00342 0.00276 0.00237 0.00176 0.00096 0.00088 0.00074\nCumulative Proportion  0.98970 0.99246 0.99483 0.99658 0.99755 0.99843 0.99917\n                          PC15    PC16      PC17\nStandard deviation     0.08716 0.08092 2.665e-07\nProportion of Variance 0.00045 0.00039 0.000e+00\nCumulative Proportion  0.99961 1.00000 1.000e+00\n\n\n\n\nScreenplot\nThe Figure 7 further confirms this decision, identifying PC2 as an elbow point.\n\n\nCode\nscreeplot(GD_pca, type = \"l\")\n\n\n\n\n\nFigure 7: Screeplot for PCs 1929-1945\n\n\n\n\n\n\nCorrelation between Gini index and PCs\nUpon analyzing the correlation between the Gini index and the principal components, we observe that a consistently negative and weak correlation with PC1 but have a tendency to become stronger. This meaning the pattern represented by PC1 is becoming more inversely related to the Gini index over time. A variable relationship with PC2: negative for 1929-1938 but turns positive for 1939-1945. Notably, 1942 and 1945 showcase a strong positive correlation, whereas 1937-1939 indicates weak or negligible correlation.\n\n\nCode\n# Extract loadings\nloadings <- GD_pca$rotation[, 1:2]  # considering only PC1 and PC2\n\n\ntransposed_loadings <- t(loadings)\ntransposed_df1 <- as.data.frame(transposed_loadings [,1:9])\nrownames(transposed_df1) <- c(\"PC1\", \"PC2\")\ncolnames(transposed_df1) <- 1929:1937\n\ntransposed_loadings <- t(loadings)\ntransposed_df2 <- as.data.frame(transposed_loadings [,10:17])\nrownames(transposed_df2) <- c(\"PC1\", \"PC2\")\ncolnames(transposed_df2) <- 1938:1945\n\n\ntransposed_df1\n\n\n          1929       1930       1931       1932       1933       1934\nPC1 -0.2349858 -0.2459327 -0.2571730 -0.2603796 -0.2616853 -0.2680236\nPC2 -0.2691690 -0.2588394 -0.1902042 -0.1824073 -0.1750844 -0.1299880\n          1935       1936        1937\nPC1 -0.2673863 -0.2687829 -0.27104249\nPC2 -0.1491159 -0.1243959 -0.08455504\n\n\nCode\ntransposed_df2\n\n\n           1938        1939       1940       1941       1942       1943\nPC1 -0.25702053 -0.26145875 -0.2445857 -0.2290794 -0.1856574 -0.1878753\nPC2 -0.01160135  0.02923975  0.1828159  0.2239589  0.3761331  0.3952901\n          1944       1945\nPC1 -0.1964641 -0.1943193\nPC2  0.4043742  0.3909881\n\n\n\n\nCorrelation biplot\nThe correlation Figure 8 reveals that all years negatively align with PC1. Besides, Early years exhibit negative correlations with PC2, mid-years are vertical, and later years are positive. In essence, PC1 primarily reflects inequality during the early World War II years. Greater inequality during this period corresponds to higher PC1 values. And PC2 embodies inequality nuances from both the Great Depression era and the mid to late World War II years.\nExamining the data further, states such as CT(Connecticut), PA(Pennsylvania), and MA(Massachusetts) exhibit pronounced inequality during the Great Depression (1929-1933), impacting their overall inequality significantly. Conversely, DE(Delaware) displayed the most significant inequality during World War II (1939-1945), with FL and NY also showing heightened inequality.\n\n\nCode\n# corrolation plot\nbiplot(GD_pca, scale = 0, cex = 0.55)\n\n\n\n\n\nFigure 8: Correlation biplot 1929-1945\n\n\n\n\n\n\nDistance biplot\nThe distance Figure 9 distinctively groups states into nearly 4 clusters.” biplot distinctively groups states into nearly 4 clusters. The closer distance between observation(States), States have more similar Gini index.\n\n\nCode\nbiplot(GD_pca, cex = 0.55)\n\n\n\n\n\nFigure 9: Distance biplot 1929-1945"
  },
  {
    "objectID": "posts/The Gini index across US  states analysis/index.html#global-financial-crisis",
    "href": "posts/The Gini index across US  states analysis/index.html#global-financial-crisis",
    "title": "The Gini index across US states analysis",
    "section": "Global Financial crisis",
    "text": "Global Financial crisis\n\nImportance of PC\nFor the data spanning 2007 to 2015, we’ve selected PC1 and PC2 based on their standard deviations exceeding 1. Combined, they account for over 98% of the total variance.\n\n\nCode\n#####  GR ##########\nGR_pca<- GR_sf %>% \n  column_to_rownames('State') %>% \n  prcomp(scale. = TRUE)\n\n summary(GR_pca)\n\n\nImportance of components:\n                          PC1    PC2     PC3     PC4     PC5     PC6     PC7\nStandard deviation     2.6788 1.3022 0.19835 0.18431 0.16314 0.11831 0.08488\nProportion of Variance 0.7974 0.1884 0.00437 0.00377 0.00296 0.00156 0.00080\nCumulative Proportion  0.7974 0.9858 0.99014 0.99391 0.99687 0.99842 0.99922\n                           PC8     PC9\nStandard deviation     0.07188 0.04269\nProportion of Variance 0.00057 0.00020\nCumulative Proportion  0.99980 1.00000\n\n\n\n\nScreenplot\nThe elbow rule applied to the Figure 10 further supports our choice, indicating that PC2 is an appropriate selection.\n\n\nCode\nscreeplot(GR_pca, type = \"l\")\n\n# Extract loadings\nloadings <- GR_pca$rotation[, 1:2]  # considering only PC1 and PC2\n\n\n\n\n\nFigure 10: Screeplot for PCs 2007-2015\n\n\n\n\n\n\nCode\ntransposed_loadings <- t(loadings)\ntransposed_df <- as.data.frame(transposed_loadings)\nrownames(transposed_df) <- c(\"PC1\", \"PC2\")\ncolnames(transposed_df) <- 2007:2015\n\n\ntransposed_df \n\n\n          2007       2008       2009       2010       2011       2012\nPC1 -0.2335881 -0.2318800 -0.3395366 -0.3648976 -0.3580511 -0.3577311\nPC2 -0.5952369 -0.5995428 -0.3048751  0.1111107  0.1798640  0.1906366\n          2013       2014       2015\nPC1 -0.3595398 -0.3586613 -0.3593801\nPC2  0.1953757  0.1934789  0.1914368\n\n\n\n\nCorrelation biplot\nUpon examining the correlation Figure 11 and the associated table, several observations stand out:\nFrom 2007 to 2009, there’s a pronounced negative linear relationship between PC2 and each respective year. This is particularly evident in 2007 and 2008, where the relationship is strongly negative, and the values of PC2 are high.\nBetween 2010 and 2015, the relationship between PC2 and the corresponding year is relatively weaker and remains negative. These years also display a comparatively lower value of PC2.\nOver the entire period, there’s a consistent negative linear relationship with PC1. PC1 and PC2 negativly affected by the Global Financial Crisis and this event lead to income inequality increase. In the post-Crisis, the inequality tend to decrease.\n\n\nCode\n# corrolation plot\nbiplot(GR_pca, scale = 0, cex = 0.8)\n\n\n\n\n\nFigure 11: Correlation biplot 2007-2015\n\n\n\n\n\n\nDistance biplot\nIn the distance Figure 12, the Euclidean spacing between states like MA(Massachusetts) and NJ(New Jersey) or NV(Nevada) and CT(Connecticut) illustrates their comparative likeness or divergence. For instance, concerning the Gini index, closely spaced states likely followed similar trajectories or maintained consistent levels of income inequality over the years.\nConversely, NY(New York), FL(Florida) and MS(Mississippi) lie far from most other states might have unique or unusual trends in income inequality across the years, making them outliers.\n\n\nCode\n# distance plot \nbiplot(GR_pca, cex = 0.8)\n\n\n\n\n\nFigure 12: Distance biplot 2007-2015"
  },
  {
    "objectID": "posts/The Gini index across US  states analysis/index.html#pca-map",
    "href": "posts/The Gini index across US  states analysis/index.html#pca-map",
    "title": "The Gini index across US states analysis",
    "section": "PCA map",
    "text": "PCA map\nFrom the plot@ref(fig:PCmap-1) we can see, the highest value of PC1 before the WW2 is in Idaho, and the lowest value is in Delaware. And the eastern region is lower than the western region. This means that the degree of inequality in the eastern region was lower than that in the western region, with New York, Delaware, and Florida being the most obvious.\n\n\n\n\n\nPC before WW2 on map\n\n\n\n\nFrom the plot@ref(fig:PCmap-2) we can see, the highest value of PC1 after the WW2 is in Indiana, and the lowest value is in New York and Florida. After World War II, we can see that lower PC values are dispersed in different peripheral regions, such as New York, Florida, and Nevada.This suggests that the peripheral regions have higher levels of inequality than the peripheral regions of the U.S.\n\n\n\n\n\nPC after WW2 on map"
  },
  {
    "objectID": "posts/The Gini index across US  states analysis/index.html#r-package-reference",
    "href": "posts/The Gini index across US  states analysis/index.html#r-package-reference",
    "title": "The Gini index across US states analysis",
    "section": "R package reference",
    "text": "R package reference\n\n\nTo cite package sf in publications, please use:\n\n  Pebesma, E., & Bivand, R. (2023). Spatial Data Science: With\n  Applications in R. Chapman and Hall/CRC.\n  https://doi.org/10.1201/9780429459016\n\n  Pebesma, E., 2018. Simple Features for R: Standardized Support for\n  Spatial Vector Data. The R Journal 10 (1), 439-446,\n  https://doi.org/10.32614/RJ-2018-009\n\nTo see these entries in BibTeX format, use 'print(<citation>,\nbibtex=TRUE)', 'toBibtex(.)', or set\n'options(citation.bibtex.max=999)'.\n\n\nTo cite package 'broom' in publications use:\n\n  Robinson D, Hayes A, Couch S (2023). _broom: Convert Statistical\n  Objects into Tidy Tibbles_. R package version 1.0.4,\n  <https://CRAN.R-project.org/package=broom>.\n\nA BibTeX entry for LaTeX users is\n\n  @Manual{,\n    title = {broom: Convert Statistical Objects into Tidy Tibbles},\n    author = {David Robinson and Alex Hayes and Simon Couch},\n    year = {2023},\n    note = {R package version 1.0.4},\n    url = {https://CRAN.R-project.org/package=broom},\n  }\n\n\nTo cite package 'tidyverse' in publications use:\n\n  Wickham H, Averick M, Bryan J, Chang W, McGowan LD, François R,\n  Grolemund G, Hayes A, Henry L, Hester J, Kuhn M, Pedersen TL, Miller\n  E, Bache SM, Müller K, Ooms J, Robinson D, Seidel DP, Spinu V,\n  Takahashi K, Vaughan D, Wilke C, Woo K, Yutani H (2019). \"Welcome to\n  the tidyverse.\" _Journal of Open Source Software_, *4*(43), 1686.\n  doi:10.21105/joss.01686 <https://doi.org/10.21105/joss.01686>.\n\nA BibTeX entry for LaTeX users is\n\n  @Article{,\n    title = {Welcome to the {tidyverse}},\n    author = {Hadley Wickham and Mara Averick and Jennifer Bryan and Winston Chang and Lucy D'Agostino McGowan and Romain François and Garrett Grolemund and Alex Hayes and Lionel Henry and Jim Hester and Max Kuhn and Thomas Lin Pedersen and Evan Miller and Stephan Milton Bache and Kirill Müller and Jeroen Ooms and David Robinson and Dana Paige Seidel and Vitalie Spinu and Kohske Takahashi and Davis Vaughan and Claus Wilke and Kara Woo and Hiroaki Yutani},\n    year = {2019},\n    journal = {Journal of Open Source Software},\n    volume = {4},\n    number = {43},\n    pages = {1686},\n    doi = {10.21105/joss.01686},\n  }\n\n\nTo cite package 'visdat' in publications use:\n\n  Tierney N (2017). \"visdat: Visualising Whole Data Frames.\" _JOSS_,\n  *2*(16), 355. doi:10.21105/joss.00355\n  <https://doi.org/10.21105/joss.00355>,\n  <http://dx.doi.org/10.21105/joss.00355>.\n\nA BibTeX entry for LaTeX users is\n\n  @Article{,\n    title = {visdat: Visualising Whole Data Frames},\n    author = {Nicholas Tierney},\n    doi = {10.21105/joss.00355},\n    url = {http://dx.doi.org/10.21105/joss.00355},\n    year = {2017},\n    publisher = {Journal of Open Source Software},\n    volume = {2},\n    number = {16},\n    pages = {355},\n    journal = {JOSS},\n  }\n\n\nTo cite ggplot2 in publications, please use\n\n  H. Wickham. ggplot2: Elegant Graphics for Data Analysis.\n  Springer-Verlag New York, 2016.\n\nA BibTeX entry for LaTeX users is\n\n  @Book{,\n    author = {Hadley Wickham},\n    title = {ggplot2: Elegant Graphics for Data Analysis},\n    publisher = {Springer-Verlag New York},\n    year = {2016},\n    isbn = {978-3-319-24277-4},\n    url = {https://ggplot2.tidyverse.org},\n  }\n\n\nTo cite package 'ggpubr' in publications use:\n\n  Kassambara A (2023). _ggpubr: 'ggplot2' Based Publication Ready\n  Plots_. R package version 0.6.0,\n  <https://CRAN.R-project.org/package=ggpubr>.\n\nA BibTeX entry for LaTeX users is\n\n  @Manual{,\n    title = {ggpubr: 'ggplot2' Based Publication Ready Plots},\n    author = {Alboukadel Kassambara},\n    year = {2023},\n    note = {R package version 0.6.0},\n    url = {https://CRAN.R-project.org/package=ggpubr},\n  }\n\n\nTo cite the MASS package in publications use:\n\n  Venables, W. N. & Ripley, B. D. (2002) Modern Applied Statistics with\n  S. Fourth Edition. Springer, New York. ISBN 0-387-95457-0\n\nA BibTeX entry for LaTeX users is\n\n  @Book{,\n    title = {Modern Applied Statistics with S},\n    author = {W. N. Venables and B. D. Ripley},\n    publisher = {Springer},\n    edition = {Fourth},\n    address = {New York},\n    year = {2002},\n    note = {ISBN 0-387-95457-0},\n    url = {https://www.stats.ox.ac.uk/pub/MASS4/},\n  }\n\n\nTo cite colorspace in publications use:\n\n  Zeileis A, Fisher JC, Hornik K, Ihaka R, McWhite CD, Murrell P,\n  Stauffer R, Wilke CO (2020). \"colorspace: A Toolbox for Manipulating\n  and Assessing Colors and Palettes.\" _Journal of Statistical\n  Software_, *96*(1), 1-49. doi:10.18637/jss.v096.i01\n  <https://doi.org/10.18637/jss.v096.i01>.\n\nIf you use HCL-based color palettes, please cite:\n\n  Zeileis A, Hornik K, Murrell P (2009). \"Escaping RGBland: Selecting\n  Colors for Statistical Graphics.\" _Computational Statistics & Data\n  Analysis_, *53*(9), 3259-3270. doi:10.1016/j.csda.2008.11.033\n  <https://doi.org/10.1016/j.csda.2008.11.033>.\n\nIf you use HCL-based color palettes in meteorological visualizations,\nplease cite:\n\n  Stauffer R, Mayr GJ, Dabernig M, Zeileis A (2009). \"Somewhere over\n  the Rainbow: How to Make Effective Use of Colors in Meteorological\n  Visualizations.\" _Bulletin of the American Meteorological Society_,\n  *96*(2), 203-216. doi:10.1175/BAMS-D-13-00155.1\n  <https://doi.org/10.1175/BAMS-D-13-00155.1>.\n\nTo see these entries in BibTeX format, use 'print(<citation>,\nbibtex=TRUE)', 'toBibtex(.)', or set\n'options(citation.bibtex.max=999)'."
  },
  {
    "objectID": "posts/Hand writting/index.html",
    "href": "posts/Hand writting/index.html",
    "title": "Handwriting Analysis",
    "section": "",
    "text": "The analysis of handwriting has always held a captivating allure, offering insights into the distinctive subtleties of human expression. Even in today’s digital age, the study of handwriting remains relevant, finding applications in fields like forensic document examination, linguistics, and personalized marketing. This report delves into the realm of handwritten letter classification by employing a combination of Principal Component Analysis (PCA) and machine learning methodologies. By harnessing these techniques, our aim is to unravel the unique variations in individuals’ letter-writing styles, ultimately contributing to a deeper comprehension of the intricate art of handwriting."
  },
  {
    "objectID": "posts/Hand writting/index.html#data-description",
    "href": "posts/Hand writting/index.html#data-description",
    "title": "Handwriting Analysis",
    "section": "Data Description",
    "text": "Data Description\nThe data is based on the EMNIST dataset that contains a 28x28 pixel image of a letter from the 26-letter Roman alphabet. You can find details about this dataset in this article.\nMy dataset is assigned a single random letter and a random subset from this dataset. Separate to this main data, the dataset will also have a new records dataset that contains five observations.\nEach row of the data is an image. Each column corresponds to one pixel value. There are 28 x 28 = 784 columns in total. The image is a single letter with a mix of upper case and lower case."
  },
  {
    "objectID": "posts/Hand writting/index.html#preliminary-analysis",
    "href": "posts/Hand writting/index.html#preliminary-analysis",
    "title": "Handwriting Analysis",
    "section": "Preliminary analysis",
    "text": "Preliminary analysis\n\nQ1 What is the letter in your data?\nThe letter in my data is ‘y’.\n\n\nQ2 Plot a random sample of 12 images of your data with the correct orientation.\n\n\nCode\nimagedata_to_plotdata <- function(data = mydata, \n                                  w = 28, \n                                  h = 28, \n                                  which = sample(1:3448, 12)) {\n  data %>% \n    mutate(id = 1:n()) %>% \n    filter(id %in% which) %>% \n    pivot_longer(starts_with(\"V\")) %>% \n    mutate(col = rep(rep(1:w, each = h), n_distinct(id)),\n           row = rep(rep(1:h, times = w), n_distinct(id)))\n}\n\ngletter <- imagedata_to_plotdata(mydata) %>% \n    ggplot(aes(col, row)) +\n    geom_tile(aes(fill = value)) + \n    facet_wrap(~id, nrow = 3) +\n    scale_y_reverse() +\n    theme_void(base_size = 18) +\n    guides(fill = \"none\") +\n    coord_equal()\ngletter\n\n\n\n\n\n\n\nQ3 Perform a principal component analysis (PCA) on your data. How much variation does the first 5 principal components explain in the data?\n\n\nCode\nmydata_pca_5 <- prcomp(mydata, rank. = 5)\nsummary(mydata_pca_5)\n\n\nImportance of first k=5 (out of 784) components:\n                            PC1      PC2       PC3       PC4       PC5\nStandard deviation     667.3433 558.4448 520.02544 418.33520 408.85848\nProportion of Variance   0.1435   0.1005   0.08712   0.05638   0.05385\nCumulative Proportion    0.1435   0.2439   0.33105   0.38742   0.44128\n\n\nWe can see that the first five principal components explain 44.128% of the variation in the data, and PC1, PC2, PC3, PC4 and PC5 explain 14.35%, 10.05%, 8.712%, 5.638% and 5.385% of the variation in the data respectively.\n\n\nQ4 Show what aspect of the data the first and second principal component loadings capture.\n\n\nCode\nmydata_pca <- prcomp(mydata)\n\npc_decompose <- function(k) {\n    Xnew <- mydata_pca$x[, k, drop =FALSE] %*% t(mydata_pca$rotation[, k, drop = TRUE])\n  rawdata <- imagedata_to_plotdata(which = sample(1:3448, 1)) \n  as.data.frame(Xnew) %>% \n    mutate(id = mydata_pca$id) %>% \n    imagedata_to_plotdata(which = sample(1:3448, 1))\n}\n\nset.seed(32649479)\ngletter %+% pc_decompose(1) + labs(title = \"PC1 loading\")\n\n\n\n\n\nCode\nset.seed(32649479)\ngletter %+% pc_decompose(2) + labs(title = \"PC2 loading\")\n\n\n\n\n\n\n\nQ5 Using the rotated data from the PCA, perform an agglomerative hierarchical clustering with average linkage.\n\n\nCode\nhaverage <- hclust(dist(mydata_pca$x), method = \"average\")\nhaverage\n\n\n\nCall:\nhclust(d = dist(mydata_pca$x), method = \"average\")\n\nCluster method   : average \nDistance         : euclidean \nNumber of objects: 3448 \n\n\n\n\nQ6 Cut the tree from question 5 to 4 clusters. Show how many observations you have per cluster.\n\n\nCode\ncave <- cutree(haverage, k = 4)\ntable(cave) %>%\n  kable(caption = \"How many observations in the 4 clusters.\") %>%\n  kable_styling()\n\n\n\n\nHow many observations in the 4 clusters.\n \n  \n    cave \n    Freq \n  \n \n\n  \n    1 \n    3441 \n  \n  \n    2 \n    4 \n  \n  \n    3 \n    2 \n  \n  \n    4 \n    1 \n  \n\n\n\n\n\n\n\nQ7 Show a sample of 10 (or the total number of images in a cluster if less than 10 observations in a cluster) images from each cluster like the plot below. What do you notice about the cluster groups?\n\n\nCode\nview_cluster <- function(k) {\n  cluster <- mydata %>% \n    mutate(group = cave) %>% \n    filter(group == k) %>% \n    imagedata_to_plotdata(which = 1:10)\n  \n  gletter %+% cluster\n}\n\np1 <- view_cluster(1) +labs(title = \"Cluster 1\")+\n  facet_wrap(~id, nrow = 5)\n\np2 <- view_cluster(2) + labs(title = \"Cluster 2\") +\n  facet_wrap(~id, nrow = 2)\n  \np3 <- view_cluster(3) + labs(title = \"Cluster 3\") +\n  facet_wrap(~id, nrow = 1)\n\np4 <- view_cluster(4) + labs(title = \"Cluster 4\") +\n  facet_wrap(~id, nrow = 1)\n\np <- p1+(p2/p3/p4)\np + plot_annotation(title = \"A sample of images from each cluster\")\n\n\n\n\n\nWe could see that the tail of the letter ‘y’ seems to be straight and the upper part more like ‘v’ in cluster 1, while the tail of the letter ‘y’ is more curved and the upper part of the letter ‘y’ is more like ‘u’ in cluster 2. Then in cluster 3, the upper part of the letter ‘y’ is narrower in width than the lower part, and finally, in cluster 4 the upper part of the letter ‘y’ is like ‘u’ and the lower part of the letter ‘y’ is like ‘v’. And the imbalance of clusters, majority of images are in one cluster, the other 3 clusters are smaller."
  },
  {
    "objectID": "posts/Hand writting/index.html#report",
    "href": "posts/Hand writting/index.html#report",
    "title": "Handwriting Analysis",
    "section": "Report",
    "text": "Report\n\nAbstract\nHandwriting analysis has consistently captivated researchers, offering a window into the intricate details of human expression. Despite the prevalence of digital communication, handwriting remains pertinent, finding utility in diverse domains such as forensic document analysis, linguistics, and personalized marketing. This report explores the landscape of handwritten letter classification through the integration of Principal Component Analysis (PCA) and advanced machine learning techniques. By harnessing these methodologies, our objective is to decode the distinct variations present in how individuals write letters, thereby enhancing our understanding of the nuanced world of handwriting.\n\n\nChoose pricipal components\nTo explore the way each person writes letters and to categorize the way they write them. I do a dimension reduction to the pixel data of a letter. And then, I produce a scree plot, and the elbow appears around the fourth to seventh PC (Principal component), therefore I choose 5 PCs (Principal components) to be used to analyze, and the 5 PCs (Principal components) explain 44.12% of the variation in the data.\n\n\nCode\ntibble(var = mydata_pca$sdev^2) %>%\n  mutate(PC = factor(1:n())) %>%\n  mutate(PC = as.numeric(PC)) %>%\n  filter(PC <= 15) %>%\n  mutate(PC = factor(PC)) %>%\n  ggplot(aes(PC, var)) +\n  geom_point() +\n  geom_line(group = 1)+\n  labs(title = \"Scree plot\")+\n  theme_bw()\n\n\n\n\n\nCode\nset.seed(32649479)\nmydata_pca_5 <- prcomp(mydata, rank. = 5)\nsummary(mydata_pca_5)\n\n\nImportance of first k=5 (out of 784) components:\n                            PC1      PC2       PC3       PC4       PC5\nStandard deviation     667.3433 558.4448 520.02544 418.33520 408.85848\nProportion of Variance   0.1435   0.1005   0.08712   0.05638   0.05385\nCumulative Proportion    0.1435   0.2439   0.33105   0.38742   0.44128\n\n\n\n\nClassify data\nI choose the k-means method to divide the different ways that a person can write a particular letter into 3 clusters. This plot shows that the letter ‘y’ is more centered in cluster 1, and the letter ‘y’ is italic, the letter tends to lean to the right in cluster 2, and the letter ‘y’ is fatter and the upper part of the letter ‘y’ resembles a ‘u’ in cluster 3.\n\n\nCode\nset.seed(32649479)\nkout <- kmeans(mydata_pca_5$x, centers = 3)\n\nview_cluster_report <- function(k) {\n  cluster <- mydata %>% \n    mutate(group = kout$cluster) %>% \n    filter(group == k) %>% \n    imagedata_to_plotdata(which = 1:20)\n  \n  gletter %+% cluster\n}\n\np1 <- view_cluster_report(1) + labs(title = \"Cluster 1\")+\n  facet_wrap(~id, nrow = 2)\n\np2 <- view_cluster_report(2) + labs(title = \"Cluster 2\")+\n  facet_wrap(~id, nrow = 2)\n\np3 <- view_cluster_report(3) + labs(title = \"Cluster 3\")+\n  facet_wrap(~id, nrow = 2)\n\np <- p1/p2/p3\np + plot_annotation(title = \"Different ways people write a letter\")\n\n\n\n\n\n\n\nCode\nimagedata_to_newrecord <- function(data = newrecord, \n                                  w = 28, \n                                  h = 28, \n                                  which = sample(1:5, 5)) {\n  data %>% \n    mutate(id = 1:n()) %>% \n    filter(id %in% which) %>% \n    pivot_longer(starts_with(\"V\")) %>% \n    mutate(col = rep(rep(1:w, each = h), n_distinct(id)),\n           row = rep(rep(1:h, times = w), n_distinct(id)))\n}\n\nnew_gletter <- imagedata_to_newrecord(newrecord) %>% \n    ggplot(aes(col, row)) +\n    geom_tile(aes(fill = value)) + \n    facet_wrap(~id, nrow = 3) +\n    scale_y_reverse() +\n    theme_void(base_size = 18) +\n    guides(fill = \"none\") +\n    coord_equal()\n\n\n\n\nClassify new records\nFor classifying a set of new record of 5 observations, I use 3 supervised learning methods, and I set principal components as predictors.\n\n\nCode\nset.seed(32649479)\nx <- mydata_pca_5$x\ncluster <- as_tibble(x) %>% \n    mutate(group = factor(kout$cluster))# create mydata's response variable, PCs as predictors.\n\nnewrecord_pca <- prcomp(newrecord)\nnewdata <- as_tibble(newrecord_pca$x) #making PCs as predictors in newdata.\n\n\nI create three models by using the kNN method, the Support vector classifier method with Polynomial kernel and the Random forest method respectively to classify the new 5 observations.\n\nkNN method\n\n\n\nCode\nset.seed(32649479)\nknn_pred <- kknn(group~.,\n                 train = cluster,\n                 test = newdata,\n                 k = 15,\n                 distance = 2) #knn\n\nview_cluster_report <- function(k) {\n  cluster <- newrecord %>% \n    mutate(group = knn_pred$fitted.values) %>% \n    filter(group == k) %>% \n    imagedata_to_newrecord(which = 1:5)\n  \n  new_gletter %+% cluster\n}\n\np1 <- view_cluster_report(1) + labs(title = \"Cluster 1\")+\n  facet_wrap(~id, nrow = 1)\n\np2 <- view_cluster_report(2) + labs(title = \"Cluster 2\")+\n  facet_wrap(~id, nrow = 1)\n\np3 <- view_cluster_report(3) + labs(title = \"Cluster 3\")+\n  facet_wrap(~id, nrow = 1)\n\npk <- p1/p2/p3\npk + plot_annotation(title = \"kNN model\") \n\n\n\n\n\n\nSupport vector classifier method with Polynomial kernel\n\n\n\nCode\nset.seed(32649479)\npoly_svm <- svm(group ~ ., data = cluster, cost =1, kernel = \"polynomial\") #svm\npred_poly <- predict(poly_svm, newdata = newdata)\n\nview_cluster_report <- function(k) {\n  cluster <- newrecord %>% \n    mutate(group = pred_poly) %>% \n    filter(group == k) %>% \n    imagedata_to_newrecord(which = 1:5)\n  \n  new_gletter %+% cluster\n}\n\np1 <- view_cluster_report(1) + labs(title = \"Cluster 1\")+\n  facet_wrap(~id, nrow = 1)\n\np2 <- view_cluster_report(2) + labs(title = \"Cluster 2\")+\n  facet_wrap(~id, nrow = 1)\n\np3 <- view_cluster_report(3) + labs(title = \"Cluster 3\")+\n  facet_wrap(~id, nrow = 1)\n\npsvm <- p1/p2/p3\npsvm + plot_annotation(title = \"Support vector classifier model\")\n\n\n\n\n\n\nRandom forest method\n\n\n\nCode\nset.seed(32649479)\nclass_rf <- ranger(group~., \n                   data = cluster,\n                   importance = \"impurity\",\n                   classification = TRUE)\n\npredict<- newdata %>%\n  mutate(group = predict(class_rf, .)$predictions)\n\nview_cluster_report <- function(k) {\n  cluster <- newrecord %>% \n    mutate(group = predict$group) %>% \n    filter(group == k) %>% \n    imagedata_to_newrecord(which = 1:5)\n  \n  new_gletter %+% cluster\n}\n\np1 <- view_cluster_report(1) + labs(title = \"Cluster 1\")+\n  facet_wrap(~id, nrow = 1)\n\np2 <- view_cluster_report(2) + labs(title = \"Cluster 2\")+\n  facet_wrap(~id, nrow = 1)\n\np3 <- view_cluster_report(3) + labs(title = \"Cluster 3\")+\n  facet_wrap(~id, nrow = 1)\n\nprf <- p1/p2/p3\nprf + plot_annotation(title = \"Random forest model\")\n\n\n\n\n\nWe can see that the classified results following the three models are the same and sensible, we could easily see that these are different 3 groups. Cluster 1 has a fatter letter, cluster 2 is more formal and centered, and cluster 3 leans a little to the right.\n\n\nCompare models\n\n\nCode\nset.seed(32649479)\n\ncluster_split <- initial_split(cluster)\ncluster_train <- training(cluster_split)\ncluster_test <- testing(cluster_split)\nknn_check <- kknn(group~.,\n                 train = cluster_train,\n                 test = cluster_test,\n                 k = 15,\n                 distance = 2) #knn\nknn_accuracy <- accuracy_vec(cluster_test$group, as.factor(knn_check$fitted.values))\n\n\npoly_svm_check <- svm(group ~ ., data = cluster_train, cost =1, kernel = \"polynomial\") #svm \npred_poly_check <- predict(poly_svm_check, newdata = cluster_test)\npoly_svm_accuracy <- accuracy_vec(cluster_test$group, as.factor(pred_poly_check))\n\n\nclass_rf_check <- ranger(group~., \n                   data = cluster_train,\n                   importance = \"impurity\",\n                   classification = TRUE) #rf\n\npredict<- cluster_test %>%\n  mutate(group = predict(class_rf_check, .)$predictions)\n\nrf_accuracy <- accuracy_vec(cluster_test$group, as.factor(predict$group))\n\ndata_frame(model = c(\"kNN\", \"Support vector classifier\", \"Random forest\"),\n                   accuracy = c(round(knn_accuracy,2), round(poly_svm_accuracy,2), round(rf_accuracy,2))) %>%\n  kable(caption = \"Accuracy of models\") %>%\n  kable_styling()\n\n\n\n\nAccuracy of models\n \n  \n    model \n    accuracy \n  \n \n\n  \n    kNN \n    0.96 \n  \n  \n    Support vector classifier \n    0.98 \n  \n  \n    Random forest \n    0.97 \n  \n\n\n\n\n\nWe could see that the accuracy of the Support vector classifier model is the best, which is 0.98.\n\n\nConclusion\nThe kNN model, the Support vector classifier model with the Polynomial kernel and the Random forest model are both good for classifying the new record of 5 observations, but following the accuracy of each model, I recommend the Support vector classifier model with the Polynomial kernel."
  },
  {
    "objectID": "posts/Alcohol/index.html",
    "href": "posts/Alcohol/index.html",
    "title": "Alcohol Consumption",
    "section": "",
    "text": "Alcohol’s role in our lives extends beyond a mere drink. It mirrors alcoholism, cultural facets, and economic dynamics. This blog delves into global alcohol consumption, spotlighting beer, spirits, and wine. These choices echo traditions, camaraderie, and refinement. Beer unites at gatherings, spirits offer mystique, and wine embodies elegance. From local pubs to international soirées, alcohol reflects diverse human experiences. This exploration unravels trends and stories, showcasing the interplay of culture and economics in our relationship with alcohol.\n\n\n image source: Wikipedia"
  }
]