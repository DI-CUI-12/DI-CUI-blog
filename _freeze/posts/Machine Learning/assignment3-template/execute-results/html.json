{
  "hash": "ffcc798b89c4ba68cc3adc0d053f1e19",
  "result": {
    "markdown": "---\ntitle: \"ETC3250/5250 IML Asignment 3 Solution\"\nauthor: Di Cui (32649479)\ndate: \"2023-08-07\"\noutput: \n  html_document:\n    code_folding: show\neditor_options: \n  chunk_output_type: console\n---\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load the packages that you will use to complete this assigment.\n\nlibrary(tidyverse)\nlibrary(kableExtra)\nlibrary(patchwork)\nlibrary(kknn)\nlibrary(yardstick)\nlibrary(rsample)\nlibrary(e1071)\nlibrary(ranger)\n\nmydata <- read.csv(\"data32649479.csv\") \nnewrecord <- read.csv(\"newrecords32649479.csv\")\n\nset.seed(32649479)\n```\n:::\n\n\n\n## Preliminary analysis\n\n### Question 1\n\nThe letter in my data is 'y'.\n\n\\newpage\n### Question 2\n\n\n::: {.cell}\n\n```{.r .cell-code}\nimagedata_to_plotdata <- function(data = mydata, \n                                  w = 28, \n                                  h = 28, \n                                  which = sample(1:3448, 12)) {\n  data %>% \n    mutate(id = 1:n()) %>% \n    filter(id %in% which) %>% \n    pivot_longer(starts_with(\"V\")) %>% \n    mutate(col = rep(rep(1:w, each = h), n_distinct(id)),\n           row = rep(rep(1:h, times = w), n_distinct(id)))\n}\n\ngletter <- imagedata_to_plotdata(mydata) %>% \n    ggplot(aes(col, row)) +\n    geom_tile(aes(fill = value)) + \n    facet_wrap(~id, nrow = 3) +\n    scale_y_reverse() +\n    theme_void(base_size = 18) +\n    guides(fill = \"none\") +\n    coord_equal()\ngletter\n```\n\n::: {.cell-output-display}\n![](assignment3-template_files/figure-html/letter-plot-1.png){width=672}\n:::\n:::\n\n\n\\newpage\n### Question 3\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmydata_pca_5 <- prcomp(mydata, rank. = 5)\nsummary(mydata_pca_5)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nImportance of first k=5 (out of 784) components:\n                            PC1      PC2       PC3       PC4       PC5\nStandard deviation     667.3433 558.4448 520.02544 418.33520 408.85848\nProportion of Variance   0.1435   0.1005   0.08712   0.05638   0.05385\nCumulative Proportion    0.1435   0.2439   0.33105   0.38742   0.44128\n```\n:::\n:::\n\n\nWe can see that the first five principal components explain 44.128% of the variation in the data, and PC1, PC2, PC3, PC4 and PC5 explain 14.35%, 10.05%, 8.712%, 5.638% and 5.385% of the variation in the data respectively.\n\n### Question 4\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmydata_pca <- prcomp(mydata)\n\npc_decompose <- function(k) {\n    Xnew <- mydata_pca$x[, k, drop = FALSE] %*% t(mydata_pca$rotation[, k, drop = FALSE])\n  rawdata <- imagedata_to_plotdata(which = sample(1:3448, 1)) \n  as.data.frame(Xnew) %>% \n    mutate(id = mydata_pca$id) %>% \n    imagedata_to_plotdata(which = sample(1:3448, 1))\n}\n\nset.seed(32649479)\ngletter %+% pc_decompose(1) + labs(title = \"PC1 loading\")\n```\n\n::: {.cell-output-display}\n![](assignment3-template_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n\n```{.r .cell-code}\nset.seed(32649479)\ngletter %+% pc_decompose(2) + labs(title = \"PC2 loading\")\n```\n\n::: {.cell-output-display}\n![](assignment3-template_files/figure-html/unnamed-chunk-3-2.png){width=672}\n:::\n:::\n\n\n### Question 5\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhaverage <- hclust(dist(mydata_pca$x), method = \"average\")\nhaverage\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nhclust(d = dist(mydata_pca$x), method = \"average\")\n\nCluster method   : average \nDistance         : euclidean \nNumber of objects: 3448 \n```\n:::\n:::\n\n\\newpage\n### Question 6\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncave <- cutree(haverage, k = 4)\ntable(cave) %>%\n  kable(caption = \"How many observations in the 4 clusters.\") %>%\n  kable_styling()\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table\" style=\"margin-left: auto; margin-right: auto;\">\n<caption>How many observations in the 4 clusters.</caption>\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> cave </th>\n   <th style=\"text-align:right;\"> Freq </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> 1 </td>\n   <td style=\"text-align:right;\"> 3441 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> 2 </td>\n   <td style=\"text-align:right;\"> 4 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> 3 </td>\n   <td style=\"text-align:right;\"> 2 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> 4 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\n### Question 7\n\n\n::: {.cell}\n\n```{.r .cell-code}\nview_cluster <- function(k) {\n  cluster <- mydata %>% \n    mutate(group = cave) %>% \n    filter(group == k) %>% \n    imagedata_to_plotdata(which = 1:10)\n  \n  gletter %+% cluster\n}\n\np1 <- view_cluster(1) +labs(title = \"Cluster 1\")+\n  facet_wrap(~id, nrow = 5)\n\np2 <- view_cluster(2) + labs(title = \"Cluster 2\") +\n  facet_wrap(~id, nrow = 2)\n  \np3 <- view_cluster(3) + labs(title = \"Cluster 3\") +\n  facet_wrap(~id, nrow = 1)\n\np4 <- view_cluster(4) + labs(title = \"Cluster 4\") +\n  facet_wrap(~id, nrow = 1)\n\np <- p1+(p2/p3/p4)\np + plot_annotation(title = \"A sample of images from each cluster\")\n```\n\n::: {.cell-output-display}\n![](assignment3-template_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\n\nWe could see that the tail of the letter 'y' seems to be straight and the upper part more like 'v' in cluster 1, while the tail of the letter 'y' is more curved and the upper part of the letter 'y' is more like 'u' in cluster 2. Then in cluster 3, the upper part of the letter 'y' is narrower in width than the lower part, and finally, in cluster 4 the upper part of the letter 'y' is like 'u' and the lower part of the letter 'y' is like 'v'.\n\n\\newpage\n\n## Report\n\n### Choose pricipal components\n\nTo explore the way each person writes letters and to categorize the way they write them. I do a dimension reduction to the pixel data of a letter. And then, I produce a scree plot, and the elbow appears around the fourth to seventh PC (Principal component), therefore I choose 5 PCs (Principal components) to be used to analyze, and the 5 PCs (Principal components) explain 44.12% of the variation in the data.\n\n\n::: {.cell}\n\n```{.r .fold-hide .cell-code}\ntibble(var = mydata_pca$sdev^2) %>%\n  mutate(PC = factor(1:n())) %>%\n  mutate(PC = as.numeric(PC)) %>%\n  filter(PC <= 15) %>%\n  mutate(PC = factor(PC)) %>%\n  ggplot(aes(PC, var)) +\n  geom_point() +\n  geom_line(group = 1)+\n  labs(title = \"Scree plot\")+\n  theme_bw()\n```\n\n::: {.cell-output-display}\n![](assignment3-template_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n\n```{.r .fold-hide .cell-code}\nset.seed(32649479)\nmydata_pca_5 <- prcomp(mydata, rank. = 5)\nsummary(mydata_pca_5)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nImportance of first k=5 (out of 784) components:\n                            PC1      PC2       PC3       PC4       PC5\nStandard deviation     667.3433 558.4448 520.02544 418.33520 408.85848\nProportion of Variance   0.1435   0.1005   0.08712   0.05638   0.05385\nCumulative Proportion    0.1435   0.2439   0.33105   0.38742   0.44128\n```\n:::\n:::\n\n\\newpage\n### Classify data\n\nI choose the k-means method to divide the different ways that a person can write a particular letter into 3 clusters. This plot shows that the letter 'y' is more centered in cluster 1, and the letter 'y' is italic, the letter tends to lean to the right in cluster 2, and the letter 'y' is fatter and the upper part of the letter 'y' resembles a 'u' in cluster 3.\n\n\n::: {.cell}\n\n```{.r .fold-hide .cell-code}\nset.seed(32649479)\nkout <- kmeans(mydata_pca_5$x, centers = 3)\n\nview_cluster_report <- function(k) {\n  cluster <- mydata %>% \n    mutate(group = kout$cluster) %>% \n    filter(group == k) %>% \n    imagedata_to_plotdata(which = 1:20)\n  \n  gletter %+% cluster\n}\n\np1 <- view_cluster_report(1) + labs(title = \"Cluster 1\")+\n  facet_wrap(~id, nrow = 2)\n\np2 <- view_cluster_report(2) + labs(title = \"Cluster 2\")+\n  facet_wrap(~id, nrow = 2)\n\np3 <- view_cluster_report(3) + labs(title = \"Cluster 3\")+\n  facet_wrap(~id, nrow = 2)\n\np <- p1/p2/p3\np + plot_annotation(title = \"Different ways people write a letter\")\n```\n\n::: {.cell-output-display}\n![](assignment3-template_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .fold-hide .cell-code}\nimagedata_to_newrecord <- function(data = newrecord, \n                                  w = 28, \n                                  h = 28, \n                                  which = sample(1:5, 5)) {\n  data %>% \n    mutate(id = 1:n()) %>% \n    filter(id %in% which) %>% \n    pivot_longer(starts_with(\"V\")) %>% \n    mutate(col = rep(rep(1:w, each = h), n_distinct(id)),\n           row = rep(rep(1:h, times = w), n_distinct(id)))\n}\n\nnew_gletter <- imagedata_to_newrecord(newrecord) %>% \n    ggplot(aes(col, row)) +\n    geom_tile(aes(fill = value)) + \n    facet_wrap(~id, nrow = 3) +\n    scale_y_reverse() +\n    theme_void(base_size = 18) +\n    guides(fill = \"none\") +\n    coord_equal()\n```\n:::\n\n\n### Classify new records\n\nFor classifying a set of new record of 5 observations, I use 3 supervised learning methods, and I set principal components as predictors.\n\n\n::: {.cell}\n\n```{.r .fold-hide .cell-code}\nset.seed(32649479)\nx <- mydata_pca_5$x\ncluster <- as_tibble(x) %>% \n    mutate(group = factor(kout$cluster))# create mydata's response variable, PCs as predictors.\n\nnewrecord_pca <- prcomp(newrecord)\nnewdata <- as_tibble(newrecord_pca$x) #making PCs as predictors in newdata.\n```\n:::\n\n\nI create three models by using the kNN method, the Support vector classifier method with Polynomial kernel and the Random forest method respectively to classify the new 5 observations. \n\n- kNN method \n\n::: {.cell}\n\n```{.r .fold-hide .cell-code}\nset.seed(32649479)\nknn_pred <- kknn(group~.,\n                 train = cluster,\n                 test = newdata,\n                 k = 15,\n                 distance = 2) #knn\n\nview_cluster_report <- function(k) {\n  cluster <- newrecord %>% \n    mutate(group = knn_pred$fitted.values) %>% \n    filter(group == k) %>% \n    imagedata_to_newrecord(which = 1:5)\n  \n  new_gletter %+% cluster\n}\n\np1 <- view_cluster_report(1) + labs(title = \"Cluster 1\")+\n  facet_wrap(~id, nrow = 1)\n\np2 <- view_cluster_report(2) + labs(title = \"Cluster 2\")+\n  facet_wrap(~id, nrow = 1)\n\np3 <- view_cluster_report(3) + labs(title = \"Cluster 3\")+\n  facet_wrap(~id, nrow = 1)\n\npk <- p1/p2/p3\npk + plot_annotation(title = \"kNN model\") \n```\n\n::: {.cell-output-display}\n![](assignment3-template_files/figure-html/unnamed-chunk-11-1.png){width=672}\n:::\n:::\n\n\n- Support vector classifier method with Polynomial kernel\n\n::: {.cell}\n\n```{.r .fold-hide .cell-code}\nset.seed(32649479)\npoly_svm <- svm(group ~ ., data = cluster, cost =1, kernel = \"polynomial\") #svm\npred_poly <- predict(poly_svm, newdata = newdata)\n\nview_cluster_report <- function(k) {\n  cluster <- newrecord %>% \n    mutate(group = pred_poly) %>% \n    filter(group == k) %>% \n    imagedata_to_newrecord(which = 1:5)\n  \n  new_gletter %+% cluster\n}\n\np1 <- view_cluster_report(1) + labs(title = \"Cluster 1\")+\n  facet_wrap(~id, nrow = 1)\n\np2 <- view_cluster_report(2) + labs(title = \"Cluster 2\")+\n  facet_wrap(~id, nrow = 1)\n\np3 <- view_cluster_report(3) + labs(title = \"Cluster 3\")+\n  facet_wrap(~id, nrow = 1)\n\npsvm <- p1/p2/p3\npsvm + plot_annotation(title = \"Support vector classifier model\")\n```\n\n::: {.cell-output-display}\n![](assignment3-template_files/figure-html/unnamed-chunk-12-1.png){width=672}\n:::\n:::\n\n\n- Random forest method\n\n::: {.cell}\n\n```{.r .fold-hide .cell-code}\nset.seed(32649479)\nclass_rf <- ranger(group~., \n                   data = cluster,\n                   importance = \"impurity\",\n                   classification = TRUE)\n\npredict<- newdata %>%\n  mutate(group = predict(class_rf, .)$predictions)\n\nview_cluster_report <- function(k) {\n  cluster <- newrecord %>% \n    mutate(group = predict$group) %>% \n    filter(group == k) %>% \n    imagedata_to_newrecord(which = 1:5)\n  \n  new_gletter %+% cluster\n}\n\np1 <- view_cluster_report(1) + labs(title = \"Cluster 1\")+\n  facet_wrap(~id, nrow = 1)\n\np2 <- view_cluster_report(2) + labs(title = \"Cluster 2\")+\n  facet_wrap(~id, nrow = 1)\n\np3 <- view_cluster_report(3) + labs(title = \"Cluster 3\")+\n  facet_wrap(~id, nrow = 1)\n\nprf <- p1/p2/p3\nprf + plot_annotation(title = \"Random forest model\")\n```\n\n::: {.cell-output-display}\n![](assignment3-template_files/figure-html/unnamed-chunk-13-1.png){width=672}\n:::\n:::\n\n\nWe can see that the classified results following the three models are the same and sensible, we could easily see that these are different 3 groups. Cluster 1 has a fatter letter, cluster 2 is more formal and centered, and cluster 3 leans a little to the right. \n\n### Compare models\n\n::: {.cell}\n\n```{.r .fold-hide .cell-code}\nset.seed(32649479)\n\ncluster_split <- initial_split(cluster)\ncluster_train <- training(cluster_split)\ncluster_test <- testing(cluster_split)\nknn_check <- kknn(group~.,\n                 train = cluster_train,\n                 test = cluster_test,\n                 k = 15,\n                 distance = 2) #knn\nknn_accuracy <- accuracy_vec(cluster_test$group, as.factor(knn_check$fitted.values))\n\n\npoly_svm_check <- svm(group ~ ., data = cluster_train, cost =1, kernel = \"polynomial\") #svm \npred_poly_check <- predict(poly_svm_check, newdata = cluster_test)\npoly_svm_accuracy <- accuracy_vec(cluster_test$group, as.factor(pred_poly_check))\n\n\nclass_rf_check <- ranger(group~., \n                   data = cluster_train,\n                   importance = \"impurity\",\n                   classification = TRUE) #rf\n\npredict<- cluster_test %>%\n  mutate(group = predict(class_rf_check, .)$predictions)\n\nrf_accuracy <- accuracy_vec(cluster_test$group, as.factor(predict$group))\n\ndata_frame(model = c(\"kNN\", \"Support vector classifier\", \"Random forest\"),\n                   accuracy = c(round(knn_accuracy,2), round(poly_svm_accuracy,2), round(rf_accuracy,2))) %>%\n  kable(caption = \"Accuracy of models\") %>%\n  kable_styling()\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table\" style=\"margin-left: auto; margin-right: auto;\">\n<caption>Accuracy of models</caption>\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> model </th>\n   <th style=\"text-align:right;\"> accuracy </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> kNN </td>\n   <td style=\"text-align:right;\"> 0.96 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Support vector classifier </td>\n   <td style=\"text-align:right;\"> 0.98 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Random forest </td>\n   <td style=\"text-align:right;\"> 0.97 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\nWe could see that the accuracy of the Support vector classifier model is the best, which is 0.98.\n\n### Conclusion\n\nThe kNN model, the Support vector classifier model with the Polynomial kernel and the Random forest model are both good for classifying the new record of 5 observations, but following the accuracy of each model, I recommend the Support vector classifier model with the Polynomial kernel.\n\n\n\n",
    "supporting": [
      "assignment3-template_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<script src=\"../../site_libs/kePrint-0.0.1/kePrint.js\"></script>\n<link href=\"../../site_libs/lightable-0.0.1/lightable.css\" rel=\"stylesheet\" />\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}